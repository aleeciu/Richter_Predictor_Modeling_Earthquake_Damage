{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sublime-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ranking-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x_train, x_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = pd.DataFrame(scaler.transform(x_train), \n",
    "                           index=x_train.index, \n",
    "                           columns=x_train.columns)\n",
    "\n",
    "    x_test = pd.DataFrame(scaler.transform(x_test),\n",
    "                          index=x_test.index, \n",
    "                          columns=x_test.columns)\n",
    "    return x_train, x_test\n",
    "\n",
    "def transform_label(y):\n",
    "    labeled_y = y.apply(lambda row: int(row.damage_grade.split('_')[1])-1, 1).values\n",
    "    return labeled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "premium-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('processed_data/x_post_preproc.csv', index_col='building_id')\n",
    "y = pd.read_csv('processed_data/y_post_preproc.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "professional-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(x, test_size=0.2, random_state=42)\n",
    "y_train, y_test = transform_label(y.loc[x_train.index]), transform_label(y.loc[x_test.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bearing-diana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 3, ..., 3, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "parliamentary-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm, x_test_norm = scale(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bacterial-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(200, input_shape=(80,), activation='relu'))\n",
    "    model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(20, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    #stochastic gradient descent\n",
    "    sgd = keras.optimizers.SGD(lr=lr)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "neither-aspect",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 609675 samples, validate on 152419 samples\n",
      "Epoch 1/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.3602 - accuracy: 0.4233 - val_loss: 1.1738 - val_accuracy: 0.4928\n",
      "Epoch 2/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1304 - accuracy: 0.5193 - val_loss: 1.1091 - val_accuracy: 0.5309\n",
      "Epoch 3/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0957 - accuracy: 0.5364 - val_loss: 1.0909 - val_accuracy: 0.5376\n",
      "Epoch 4/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0830 - accuracy: 0.5413 - val_loss: 1.0820 - val_accuracy: 0.5412\n",
      "Epoch 5/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0759 - accuracy: 0.5447 - val_loss: 1.0766 - val_accuracy: 0.5428\n",
      "Epoch 6/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0711 - accuracy: 0.5464 - val_loss: 1.0725 - val_accuracy: 0.5441\n",
      "Epoch 7/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0674 - accuracy: 0.5481 - val_loss: 1.0690 - val_accuracy: 0.5462\n",
      "Epoch 8/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0644 - accuracy: 0.5493 - val_loss: 1.0666 - val_accuracy: 0.5471\n",
      "Epoch 9/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0619 - accuracy: 0.5503 - val_loss: 1.0639 - val_accuracy: 0.5484\n",
      "Epoch 10/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0597 - accuracy: 0.5513 - val_loss: 1.0619 - val_accuracy: 0.5498\n",
      "Epoch 11/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0576 - accuracy: 0.5521 - val_loss: 1.0607 - val_accuracy: 0.5497\n",
      "Epoch 12/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0558 - accuracy: 0.5529 - val_loss: 1.0589 - val_accuracy: 0.5507\n",
      "Epoch 13/100\n",
      "609675/609675 [==============================] - 17s 27us/sample - loss: 1.0542 - accuracy: 0.5534 - val_loss: 1.0571 - val_accuracy: 0.5512\n",
      "Epoch 14/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0526 - accuracy: 0.5541 - val_loss: 1.0556 - val_accuracy: 0.5522\n",
      "Epoch 15/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0512 - accuracy: 0.5548 - val_loss: 1.0545 - val_accuracy: 0.5523\n",
      "Epoch 16/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0499 - accuracy: 0.5554 - val_loss: 1.0532 - val_accuracy: 0.5532\n",
      "Epoch 17/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0486 - accuracy: 0.5558 - val_loss: 1.0520 - val_accuracy: 0.5535\n",
      "Epoch 18/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0474 - accuracy: 0.5566 - val_loss: 1.0511 - val_accuracy: 0.5537\n",
      "Epoch 19/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0462 - accuracy: 0.5570 - val_loss: 1.0505 - val_accuracy: 0.5546\n",
      "Epoch 20/100\n",
      "609675/609675 [==============================] - 20s 32us/sample - loss: 1.0451 - accuracy: 0.5574 - val_loss: 1.0491 - val_accuracy: 0.5552\n",
      "Epoch 21/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0440 - accuracy: 0.5581 - val_loss: 1.0482 - val_accuracy: 0.5553\n",
      "Epoch 22/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0430 - accuracy: 0.5585 - val_loss: 1.0472 - val_accuracy: 0.5556\n",
      "Epoch 23/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0420 - accuracy: 0.5590 - val_loss: 1.0462 - val_accuracy: 0.5562\n",
      "Epoch 24/100\n",
      "609675/609675 [==============================] - 19s 30us/sample - loss: 1.0410 - accuracy: 0.5594 - val_loss: 1.0456 - val_accuracy: 0.5566\n",
      "Epoch 25/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0401 - accuracy: 0.5596 - val_loss: 1.0448 - val_accuracy: 0.5566\n",
      "Epoch 26/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0392 - accuracy: 0.5600 - val_loss: 1.0443 - val_accuracy: 0.5573\n",
      "Epoch 27/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0383 - accuracy: 0.5607 - val_loss: 1.0432 - val_accuracy: 0.5571\n",
      "Epoch 28/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0374 - accuracy: 0.5610 - val_loss: 1.0426 - val_accuracy: 0.5573\n",
      "Epoch 29/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0367 - accuracy: 0.5611 - val_loss: 1.0420 - val_accuracy: 0.5580\n",
      "Epoch 30/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0359 - accuracy: 0.5615 - val_loss: 1.0414 - val_accuracy: 0.5582\n",
      "Epoch 31/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0351 - accuracy: 0.5619 - val_loss: 1.0409 - val_accuracy: 0.5578\n",
      "Epoch 32/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0344 - accuracy: 0.5621 - val_loss: 1.0404 - val_accuracy: 0.5589\n",
      "Epoch 33/100\n",
      "609675/609675 [==============================] - 17s 27us/sample - loss: 1.0337 - accuracy: 0.5626 - val_loss: 1.0397 - val_accuracy: 0.5585\n",
      "Epoch 34/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0329 - accuracy: 0.5628 - val_loss: 1.0393 - val_accuracy: 0.5591\n",
      "Epoch 35/100\n",
      "609675/609675 [==============================] - 17s 27us/sample - loss: 1.0323 - accuracy: 0.5631 - val_loss: 1.0389 - val_accuracy: 0.5588\n",
      "Epoch 36/100\n",
      "609675/609675 [==============================] - 17s 27us/sample - loss: 1.0316 - accuracy: 0.5633 - val_loss: 1.0386 - val_accuracy: 0.5590\n",
      "Epoch 37/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0309 - accuracy: 0.5637 - val_loss: 1.0377 - val_accuracy: 0.5594\n",
      "Epoch 38/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0303 - accuracy: 0.5638 - val_loss: 1.0372 - val_accuracy: 0.5599\n",
      "Epoch 39/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0296 - accuracy: 0.5642 - val_loss: 1.0369 - val_accuracy: 0.5598\n",
      "Epoch 40/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0290 - accuracy: 0.5646 - val_loss: 1.0363 - val_accuracy: 0.5599\n",
      "Epoch 41/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0284 - accuracy: 0.5647 - val_loss: 1.0362 - val_accuracy: 0.5606\n",
      "Epoch 42/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0278 - accuracy: 0.5652 - val_loss: 1.0355 - val_accuracy: 0.5601\n",
      "Epoch 43/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0272 - accuracy: 0.5652 - val_loss: 1.0352 - val_accuracy: 0.5608\n",
      "Epoch 44/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0266 - accuracy: 0.5654 - val_loss: 1.0348 - val_accuracy: 0.5609\n",
      "Epoch 45/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0261 - accuracy: 0.5659 - val_loss: 1.0342 - val_accuracy: 0.5612\n",
      "Epoch 46/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0255 - accuracy: 0.5659 - val_loss: 1.0342 - val_accuracy: 0.5613\n",
      "Epoch 47/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0250 - accuracy: 0.5662 - val_loss: 1.0337 - val_accuracy: 0.5612\n",
      "Epoch 48/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0245 - accuracy: 0.5664 - val_loss: 1.0331 - val_accuracy: 0.5617\n",
      "Epoch 49/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0240 - accuracy: 0.5668 - val_loss: 1.0331 - val_accuracy: 0.5612\n",
      "Epoch 50/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0235 - accuracy: 0.5668 - val_loss: 1.0325 - val_accuracy: 0.5619\n",
      "Epoch 51/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0230 - accuracy: 0.5672 - val_loss: 1.0321 - val_accuracy: 0.5622\n",
      "Epoch 52/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0225 - accuracy: 0.5674 - val_loss: 1.0317 - val_accuracy: 0.5620\n",
      "Epoch 53/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0220 - accuracy: 0.5676 - val_loss: 1.0316 - val_accuracy: 0.5623\n",
      "Epoch 54/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0215 - accuracy: 0.5680 - val_loss: 1.0319 - val_accuracy: 0.5622\n",
      "Epoch 55/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0210 - accuracy: 0.5681 - val_loss: 1.0309 - val_accuracy: 0.5624\n",
      "Epoch 56/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0205 - accuracy: 0.5682 - val_loss: 1.0308 - val_accuracy: 0.5623\n",
      "Epoch 57/100\n",
      "609675/609675 [==============================] - 19s 32us/sample - loss: 1.0201 - accuracy: 0.5682 - val_loss: 1.0310 - val_accuracy: 0.5626\n",
      "Epoch 58/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0196 - accuracy: 0.5685 - val_loss: 1.0302 - val_accuracy: 0.5627\n",
      "Epoch 59/100\n",
      "609675/609675 [==============================] - 19s 30us/sample - loss: 1.0192 - accuracy: 0.5686 - val_loss: 1.0304 - val_accuracy: 0.5627\n",
      "Epoch 60/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0188 - accuracy: 0.5689 - val_loss: 1.0302 - val_accuracy: 0.5633\n",
      "Epoch 61/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0183 - accuracy: 0.5688 - val_loss: 1.0295 - val_accuracy: 0.5636\n",
      "Epoch 62/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0179 - accuracy: 0.5692 - val_loss: 1.0289 - val_accuracy: 0.5635\n",
      "Epoch 63/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0174 - accuracy: 0.5694 - val_loss: 1.0288 - val_accuracy: 0.5632\n",
      "Epoch 64/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0170 - accuracy: 0.5695 - val_loss: 1.0286 - val_accuracy: 0.5636\n",
      "Epoch 65/100\n",
      "609675/609675 [==============================] - 19s 32us/sample - loss: 1.0166 - accuracy: 0.5695 - val_loss: 1.0282 - val_accuracy: 0.5636\n",
      "Epoch 66/100\n",
      "609675/609675 [==============================] - 19s 31us/sample - loss: 1.0162 - accuracy: 0.5698 - val_loss: 1.0278 - val_accuracy: 0.5637\n",
      "Epoch 67/100\n",
      "609675/609675 [==============================] - 19s 32us/sample - loss: 1.0158 - accuracy: 0.5702 - val_loss: 1.0276 - val_accuracy: 0.5638\n",
      "Epoch 68/100\n",
      "609675/609675 [==============================] - 21s 35us/sample - loss: 1.0153 - accuracy: 0.5701 - val_loss: 1.0274 - val_accuracy: 0.5638\n",
      "Epoch 69/100\n",
      "609675/609675 [==============================] - 17s 29us/sample - loss: 1.0150 - accuracy: 0.5705 - val_loss: 1.0272 - val_accuracy: 0.5637\n",
      "Epoch 70/100\n",
      "609675/609675 [==============================] - 17s 29us/sample - loss: 1.0146 - accuracy: 0.5705 - val_loss: 1.0272 - val_accuracy: 0.5637\n",
      "Epoch 71/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0142 - accuracy: 0.5706 - val_loss: 1.0267 - val_accuracy: 0.5636\n",
      "Epoch 72/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0138 - accuracy: 0.5707 - val_loss: 1.0268 - val_accuracy: 0.5638\n",
      "Epoch 73/100\n",
      "609675/609675 [==============================] - 17s 27us/sample - loss: 1.0135 - accuracy: 0.5709 - val_loss: 1.0264 - val_accuracy: 0.5644\n",
      "Epoch 74/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0131 - accuracy: 0.5711 - val_loss: 1.0261 - val_accuracy: 0.5637\n",
      "Epoch 75/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0127 - accuracy: 0.5713 - val_loss: 1.0263 - val_accuracy: 0.5636\n",
      "Epoch 76/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0123 - accuracy: 0.5715 - val_loss: 1.0256 - val_accuracy: 0.5638\n",
      "Epoch 77/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0120 - accuracy: 0.5715 - val_loss: 1.0258 - val_accuracy: 0.5641\n",
      "Epoch 78/100\n",
      "609675/609675 [==============================] - 17s 27us/sample - loss: 1.0116 - accuracy: 0.5718 - val_loss: 1.0259 - val_accuracy: 0.5639\n",
      "Epoch 79/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0113 - accuracy: 0.5717 - val_loss: 1.0254 - val_accuracy: 0.5639\n",
      "Epoch 80/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0109 - accuracy: 0.5721 - val_loss: 1.0252 - val_accuracy: 0.5639\n",
      "Epoch 81/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0106 - accuracy: 0.5720 - val_loss: 1.0249 - val_accuracy: 0.5642\n",
      "Epoch 82/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0102 - accuracy: 0.5723 - val_loss: 1.0248 - val_accuracy: 0.5641\n",
      "Epoch 83/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0098 - accuracy: 0.5723 - val_loss: 1.0245 - val_accuracy: 0.5647\n",
      "Epoch 84/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0095 - accuracy: 0.5723 - val_loss: 1.0244 - val_accuracy: 0.5647\n",
      "Epoch 85/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0092 - accuracy: 0.5728 - val_loss: 1.0239 - val_accuracy: 0.5645\n",
      "Epoch 86/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0089 - accuracy: 0.5730 - val_loss: 1.0240 - val_accuracy: 0.5643\n",
      "Epoch 87/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0085 - accuracy: 0.5730 - val_loss: 1.0246 - val_accuracy: 0.5644\n",
      "Epoch 88/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0082 - accuracy: 0.5730 - val_loss: 1.0237 - val_accuracy: 0.5645\n",
      "Epoch 89/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0079 - accuracy: 0.5734 - val_loss: 1.0237 - val_accuracy: 0.5648\n",
      "Epoch 90/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0076 - accuracy: 0.5735 - val_loss: 1.0231 - val_accuracy: 0.5644\n",
      "Epoch 91/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0073 - accuracy: 0.5734 - val_loss: 1.0234 - val_accuracy: 0.5647\n",
      "Epoch 92/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0070 - accuracy: 0.5736 - val_loss: 1.0229 - val_accuracy: 0.5652\n",
      "Epoch 93/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0066 - accuracy: 0.5738 - val_loss: 1.0230 - val_accuracy: 0.5644\n",
      "Epoch 94/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0063 - accuracy: 0.5739 - val_loss: 1.0230 - val_accuracy: 0.5645\n",
      "Epoch 95/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0061 - accuracy: 0.5737 - val_loss: 1.0235 - val_accuracy: 0.5649\n",
      "Epoch 96/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0057 - accuracy: 0.5741 - val_loss: 1.0222 - val_accuracy: 0.5651\n",
      "Epoch 97/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0054 - accuracy: 0.5742 - val_loss: 1.0223 - val_accuracy: 0.5654\n",
      "Epoch 98/100\n",
      "609675/609675 [==============================] - 18s 29us/sample - loss: 1.0051 - accuracy: 0.5744 - val_loss: 1.0226 - val_accuracy: 0.5655\n",
      "Epoch 99/100\n",
      "609675/609675 [==============================] - 18s 30us/sample - loss: 1.0048 - accuracy: 0.5746 - val_loss: 1.0223 - val_accuracy: 0.5645\n",
      "Epoch 100/100\n",
      "609675/609675 [==============================] - 17s 28us/sample - loss: 1.0046 - accuracy: 0.5745 - val_loss: 1.0218 - val_accuracy: 0.5648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x218887d5108>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.001\n",
    "epochs=100\n",
    "batch_size=100\n",
    "\n",
    "model = create_nn(lr)\n",
    "\n",
    "model.fit(x_train_norm, y_train, \n",
    "           epochs=epochs, batch_size=batch_size, \n",
    "           validation_data=(x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "written-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152419/152419 [==============================] - 5s 33us/sample - loss: 1.0218 - accuracy: 0.5648s - loss: 1.0222 - accuracy: 0.56 - ETA: 1s - loss: 1.0217 \n",
      "\n",
      "accuracy: 56.48%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test_norm, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "local-positive",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 609675 samples, validate on 152419 samples\n",
      "Epoch 1/100\n",
      "609675/609675 [==============================] - 17s 27us/sample - loss: 1.5451 - accuracy: 0.3670 - val_loss: 1.4790 - val_accuracy: 0.4068\n",
      "Epoch 2/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.4166 - accuracy: 0.4136 - val_loss: 1.3725 - val_accuracy: 0.4170\n",
      "Epoch 3/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.3394 - accuracy: 0.4216 - val_loss: 1.3189 - val_accuracy: 0.4226\n",
      "Epoch 4/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.2960 - accuracy: 0.4265 - val_loss: 1.2834 - val_accuracy: 0.4268loss: 1.3020 - accuracy: 0. - ETA: 6s - loss: 1.3022 - ac - ETA: 2s - l - ETA: 0s - loss: 1.2963 - accuracy\n",
      "Epoch 5/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.2651 - accuracy: 0.4301 - val_loss: 1.2566 - val_accuracy: 0.4303 ETA: 4s - loss: 1.2690 - ac - ETA: 3s - loss: 1.2684 - ac - ETA: 3s - loss: 1.2 - - ETA: 0s - loss: 1.2660 - accuracy: 0. - ETA: 0s - loss: 1.2660 - accuracy:  - ETA: 0s - loss: 1.2657 - accuracy:  - ETA: 0s - loss: 1.2656 - accuracy - ETA: 0s - loss: 1.2654 - accu\n",
      "Epoch 6/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.2413 - accuracy: 0.4337 - val_loss: 1.2356 - val_accuracy: 0.4337oss: 1.2419 - ac - ETA: 0s - loss: 1.2416 - ac - ETA: 0s - loss: 1.2413 - accuracy: 0.\n",
      "Epoch 7/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.2225 - accuracy: 0.4391 - val_loss: 1.2189 - val_accuracy: 0.4394\n",
      "Epoch 8/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.2075 - accuracy: 0.4463 - val_loss: 1.2054 - val_accuracy: 0.4474 - los\n",
      "Epoch 9/100\n",
      "609675/609675 [==============================] - 14s 24us/sample - loss: 1.1953 - accuracy: 0.4562 - val_loss: 1.1943 - val_accuracy: 0.4604\n",
      "Epoch 10/100\n",
      "609675/609675 [==============================] - 14s 24us/sample - loss: 1.1851 - accuracy: 0.4752 - val_loss: 1.1848 - val_accuracy: 0.4826\n",
      "Epoch 11/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1762 - accuracy: 0.4952 - val_loss: 1.1763 - val_accuracy: 0.4997\n",
      "Epoch 12/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1681 - accuracy: 0.5065 - val_loss: 1.1686 - val_accuracy: 0.5049\n",
      "Epoch 13/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1608 - accuracy: 0.5095 - val_loss: 1.1613 - val_accuracy: 0.5070\n",
      "Epoch 14/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1536 - accuracy: 0.5111 - val_loss: 1.1541 - val_accuracy: 0.5093 - ETA: 1s - loss: 1.154 - ETA: 0s - l\n",
      "Epoch 15/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1463 - accuracy: 0.5137 - val_loss: 1.1466 - val_accuracy: 0.5120\n",
      "Epoch 16/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.1387 - accuracy: 0.5170 - val_loss: 1.1389 - val_accuracy: 0.5168\n",
      "Epoch 17/100\n",
      "609675/609675 [==============================] - 15s 24us/sample - loss: 1.1310 - accuracy: 0.5219 - val_loss: 1.1312 - val_accuracy: 0.5224\n",
      "Epoch 18/100\n",
      "609675/609675 [==============================] - 15s 24us/sample - loss: 1.1236 - accuracy: 0.5266 - val_loss: 1.1241 - val_accuracy: 0.5264\n",
      "Epoch 19/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1170 - accuracy: 0.5299 - val_loss: 1.1180 - val_accuracy: 0.5288\n",
      "Epoch 20/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1116 - accuracy: 0.5318 - val_loss: 1.1131 - val_accuracy: 0.5303: 2s - loss: 1.1126 - accura - ETA: 1s - loss: 1.1116 - accu - ETA: 0s - loss: 1.1116 - accuracy:  - ETA: 0s - loss: 1.1117 - accuracy: 0. - ETA: 0s - loss:\n",
      "Epoch 21/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1072 - accuracy: 0.5329 - val_loss: 1.1091 - val_accuracy: 0.5315 1.1072 - accuracy: 0.53\n",
      "Epoch 22/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1036 - accuracy: 0.5338 - val_loss: 1.1059 - val_accuracy: 0.5322- loss: 1.103 - ETA: 8s - loss: 1 - ETA: 7s - los - ETA: 6s - loss: 1.1040 - accuracy: 0.53 - - ETA: 5s - loss: 1.1042 - accuracy:  - ETA: 5s - loss: 1.1 - ETA: 4s - loss: 1.1 - ETA: 4s - loss: 1 - ETA: 1s - l - ETA: 0s - l\n",
      "Epoch 23/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.1007 - accuracy: 0.5346 - val_loss: 1.1032 - val_accuracy: 0.5327oss: 1.1009  - ETA: 0s - loss: 1.1\n",
      "Epoch 24/100\n",
      "609675/609675 [==============================] - 15s 24us/sample - loss: 1.0982 - accuracy: 0.5351 - val_loss: 1.1009 - val_accuracy: 0.5332\n",
      "Epoch 25/100\n",
      "609675/609675 [==============================] - 14s 24us/sample - loss: 1.0961 - accuracy: 0.5358 - val_loss: 1.0988 - val_accuracy: 0.5341\n",
      "Epoch 26/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0942 - accuracy: 0.5364 - val_loss: 1.0971 - val_accuracy: 0.5349 los - ETA: 0s - loss:\n",
      "Epoch 27/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0925 - accuracy: 0.5368 - val_loss: 1.0955 - val_accuracy: 0.5354\n",
      "Epoch 28/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0911 - accuracy: 0.5373 - val_loss: 1.0941 - val_accuracy: 0.5358.0928 - accuracy:  - ETA: 4s - loss: 1.0927 - accuracy - ETA: 3s - loss: 1.092 - - ETA: 1s - loss: 1 - ETA: 0s - loss: 1.091\n",
      "Epoch 29/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0897 - accuracy: 0.5376 - val_loss: 1.0928 - val_accuracy: 0.5365\n",
      "Epoch 30/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0885 - accuracy: 0.5381 - val_loss: 1.0916 - val_accuracy: 0.5365\n",
      "Epoch 31/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0874 - accuracy: 0.5386 - val_loss: 1.0906 - val_accuracy: 0.5369oss: 1.087\n",
      "Epoch 32/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0864 - accuracy: 0.5391 - val_loss: 1.0896 - val_accuracy: 0.5372\n",
      "Epoch 33/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0854 - accuracy: 0.5394 - val_loss: 1.0887 - val_accuracy: 0.5378\n",
      "Epoch 34/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0845 - accuracy: 0.5399 - val_loss: 1.0877 - val_accuracy: 0.5379- accu - ETA: 2s - loss: 1.0848  - ETA: 1s - loss: 1 - ETA - ETA: 0s - loss: 1.0845 - accuracy: \n",
      "Epoch 35/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0836 - accuracy: 0.5400 - val_loss: 1.0869 - val_accuracy: 0.5384\n",
      "Epoch 36/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0828 - accuracy: 0.5404 - val_loss: 1.0861 - val_accuracy: 0.5388ETA: 1s - loss: 1.0826 - accuracy: 0.54 - ETA:  - ETA: 0s - loss: 1.0\n",
      "Epoch 37/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0820 - accuracy: 0.5407 - val_loss: 1.0853 - val_accuracy: 0.5391\n",
      "Epoch 38/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0813 - accuracy: 0.5410 - val_loss: 1.0846 - val_accuracy: 0.5394\n",
      "Epoch 39/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0806 - accuracy: 0.5413 - val_loss: 1.0839 - val_accuracy: 0.5393.0802 - accuracy - ETA: 2s - loss: 1.0800 - accuracy: 0.54\n",
      "Epoch 40/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0799 - accuracy: 0.5415 - val_loss: 1.0832 - val_accuracy: 0.5397\n",
      "Epoch 41/100\n",
      "609675/609675 [==============================] - 16s 27us/sample - loss: 1.0792 - accuracy: 0.5420 - val_loss: 1.0826 - val_accuracy: 0.5401\n",
      "Epoch 42/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0786 - accuracy: 0.5422 - val_loss: 1.0820 - val_accuracy: 0.5403oss: 1.0787 - accuracy:  - ETA: 5s - loss: 1.0789 - accuracy:  - ETA:  - ETA: 4s - l - ETA: 3s - loss: - ETA: 0s - loss: 1\n",
      "Epoch 43/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0780 - accuracy: 0.5425 - val_loss: 1.0814 - val_accuracy: 0.5405\n",
      "Epoch 44/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0774 - accuracy: 0.5428 - val_loss: 1.0808 - val_accuracy: 0.5406\n",
      "Epoch 45/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0768 - accuracy: 0.5431 - val_loss: 1.0802 - val_accuracy: 0.5408\n",
      "Epoch 46/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0763 - accuracy: 0.5434 - val_loss: 1.0797 - val_accuracy: 0.5411\n",
      "Epoch 47/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0758 - accuracy: 0.5436 - val_loss: 1.0791 - val_accuracy: 0.5413\n",
      "Epoch 48/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0752 - accuracy: 0.5439 - val_loss: 1.0786 - val_accuracy: 0.5414\n",
      "Epoch 49/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0747 - accuracy: 0.5440 - val_loss: 1.0781 - val_accuracy: 0.5417\n",
      "Epoch 50/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0742 - accuracy: 0.5442 - val_loss: 1.0776 - val_accuracy: 0.5419\n",
      "Epoch 51/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0738 - accuracy: 0.5446 - val_loss: 1.0772 - val_accuracy: 0.5418\n",
      "Epoch 52/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0733 - accuracy: 0.5448 - val_loss: 1.0767 - val_accuracy: 0.5421\n",
      "Epoch 53/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0728 - accuracy: 0.5449 - val_loss: 1.0763 - val_accuracy: 0.5423\n",
      "Epoch 54/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0724 - accuracy: 0.5451 - val_loss: 1.0758 - val_accuracy: 0.5425\n",
      "Epoch 55/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0720 - accuracy: 0.5453 - val_loss: 1.0754 - val_accuracy: 0.5426\n",
      "Epoch 56/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0716 - accuracy: 0.5455 - val_loss: 1.0750 - val_accuracy: 0.5428\n",
      "Epoch 57/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0712 - accuracy: 0.5456 - val_loss: 1.0746 - val_accuracy: 0.5428\n",
      "Epoch 58/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0708 - accuracy: 0.5458 - val_loss: 1.0742 - val_accuracy: 0.5428\n",
      "Epoch 59/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0704 - accuracy: 0.5459 - val_loss: 1.0738 - val_accuracy: 0.5429\n",
      "Epoch 60/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0700 - accuracy: 0.5461 - val_loss: 1.0734 - val_accuracy: 0.5431\n",
      "Epoch 61/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0696 - accuracy: 0.5462 - val_loss: 1.0731 - val_accuracy: 0.5432oss:\n",
      "Epoch 62/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0693 - accuracy: 0.5464 - val_loss: 1.0728 - val_accuracy: 0.5432\n",
      "Epoch 63/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0689 - accuracy: 0.5466 - val_loss: 1.0724 - val_accuracy: 0.5436\n",
      "Epoch 64/100\n",
      "609675/609675 [==============================] - 16s 25us/sample - loss: 1.0686 - accuracy: 0.5467 - val_loss: 1.0721 - val_accuracy: 0.5437\n",
      "Epoch 65/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0682 - accuracy: 0.5469 - val_loss: 1.0718 - val_accuracy: 0.5440\n",
      "Epoch 66/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0679 - accuracy: 0.5471 - val_loss: 1.0714 - val_accuracy: 0.5438\n",
      "Epoch 67/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0676 - accuracy: 0.5474 - val_loss: 1.0711 - val_accuracy: 0.5440\n",
      "Epoch 68/100\n",
      "609675/609675 [==============================] - 16s 25us/sample - loss: 1.0673 - accuracy: 0.5473 - val_loss: 1.0708 - val_accuracy: 0.5441\n",
      "Epoch 69/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0670 - accuracy: 0.5476 - val_loss: 1.0705 - val_accuracy: 0.5442\n",
      "Epoch 70/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0667 - accuracy: 0.5478 - val_loss: 1.0702 - val_accuracy: 0.5442\n",
      "Epoch 71/100\n",
      "609675/609675 [==============================] - 14s 23us/sample - loss: 1.0664 - accuracy: 0.5479 - val_loss: 1.0699 - val_accuracy: 0.5444\n",
      "Epoch 72/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0661 - accuracy: 0.5480 - val_loss: 1.0696 - val_accuracy: 0.5448\n",
      "Epoch 73/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0658 - accuracy: 0.5482 - val_loss: 1.0694 - val_accuracy: 0.5447\n",
      "Epoch 74/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0655 - accuracy: 0.5483 - val_loss: 1.0691 - val_accuracy: 0.5448\n",
      "Epoch 75/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0652 - accuracy: 0.5483 - val_loss: 1.0689 - val_accuracy: 0.5450\n",
      "Epoch 76/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0650 - accuracy: 0.5485 - val_loss: 1.0686 - val_accuracy: 0.5452\n",
      "Epoch 77/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0647 - accuracy: 0.5486 - val_loss: 1.0683 - val_accuracy: 0.5453\n",
      "Epoch 78/100\n",
      "609675/609675 [==============================] - 15s 24us/sample - loss: 1.0645 - accuracy: 0.5486 - val_loss: 1.0681 - val_accuracy: 0.5454\n",
      "Epoch 79/100\n",
      "609675/609675 [==============================] - 16s 25us/sample - loss: 1.0642 - accuracy: 0.5489 - val_loss: 1.0679 - val_accuracy: 0.5454\n",
      "Epoch 80/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0640 - accuracy: 0.5489 - val_loss: 1.0676 - val_accuracy: 0.5457\n",
      "Epoch 81/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0637 - accuracy: 0.5490 - val_loss: 1.0674 - val_accuracy: 0.5455\n",
      "Epoch 82/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0635 - accuracy: 0.5492 - val_loss: 1.0672 - val_accuracy: 0.5456\n",
      "Epoch 83/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0632 - accuracy: 0.5493 - val_loss: 1.0669 - val_accuracy: 0.5461\n",
      "Epoch 84/100\n",
      "609675/609675 [==============================] - 15s 24us/sample - loss: 1.0630 - accuracy: 0.5494 - val_loss: 1.0667 - val_accuracy: 0.5461\n",
      "Epoch 85/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0628 - accuracy: 0.5495 - val_loss: 1.0665 - val_accuracy: 0.5459\n",
      "Epoch 86/100\n",
      "609675/609675 [==============================] - 15s 24us/sample - loss: 1.0625 - accuracy: 0.5496 - val_loss: 1.0663 - val_accuracy: 0.5462\n",
      "Epoch 87/100\n",
      "609675/609675 [==============================] - 14s 23us/sample - loss: 1.0623 - accuracy: 0.5498 - val_loss: 1.0661 - val_accuracy: 0.5463\n",
      "Epoch 88/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0621 - accuracy: 0.5498 - val_loss: 1.0659 - val_accuracy: 0.5464\n",
      "Epoch 89/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0619 - accuracy: 0.5499 - val_loss: 1.0657 - val_accuracy: 0.5463\n",
      "Epoch 90/100\n",
      "609675/609675 [==============================] - 16s 25us/sample - loss: 1.0616 - accuracy: 0.5499 - val_loss: 1.0655 - val_accuracy: 0.5466\n",
      "Epoch 91/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0614 - accuracy: 0.5500 - val_loss: 1.0653 - val_accuracy: 0.5465- accuracy:  - ETA: 0s - loss: 1.0613 - ac - ETA: 0s - loss: 1.0614 - accuracy: 0.\n",
      "Epoch 92/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0612 - accuracy: 0.5500 - val_loss: 1.0651 - val_accuracy: 0.5468\n",
      "Epoch 93/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0610 - accuracy: 0.5501 - val_loss: 1.0649 - val_accuracy: 0.5467610 - accu\n",
      "Epoch 94/100\n",
      "609675/609675 [==============================] - 14s 23us/sample - loss: 1.0608 - accuracy: 0.5501 - val_loss: 1.0647 - val_accuracy: 0.5467\n",
      "Epoch 95/100\n",
      "609675/609675 [==============================] - 15s 24us/sample - loss: 1.0606 - accuracy: 0.5502 - val_loss: 1.0645 - val_accuracy: 0.5468\n",
      "Epoch 96/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0604 - accuracy: 0.5503 - val_loss: 1.0643 - val_accuracy: 0.5473\n",
      "Epoch 97/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0602 - accuracy: 0.5503 - val_loss: 1.0641 - val_accuracy: 0.5468\n",
      "Epoch 98/100\n",
      "609675/609675 [==============================] - 16s 25us/sample - loss: 1.0600 - accuracy: 0.5504 - val_loss: 1.0640 - val_accuracy: 0.5469\n",
      "Epoch 99/100\n",
      "609675/609675 [==============================] - 15s 25us/sample - loss: 1.0598 - accuracy: 0.5505 - val_loss: 1.0638 - val_accuracy: 0.5470\n",
      "Epoch 100/100\n",
      "609675/609675 [==============================] - 16s 26us/sample - loss: 1.0597 - accuracy: 0.5507 - val_loss: 1.0636 - val_accuracy: 0.5473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x218841eb5c8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.0001\n",
    "epochs=100\n",
    "batch_size=100\n",
    "\n",
    "model1 = create_nn(lr)\n",
    "\n",
    "model1.fit(x_train_norm, y_train, \n",
    "           epochs=epochs, batch_size=batch_size, \n",
    "           validation_data=(x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-prince",
   "metadata": {},
   "source": [
    "Decreasing lr seems decreasing performance. Let's trying adding more layers to the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "international-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(500, input_shape=(80,), activation='relu'))\n",
    "    model.add(keras.layers.Dense(200, activation='relu'))\n",
    "    model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(20, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    #stochastic gradient descent\n",
    "    sgd = keras.optimizers.SGD(lr=lr)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "incomplete-warrior",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 609675 samples, validate on 152419 samples\n",
      "Epoch 1/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.3088 - accuracy: 0.4410 - val_loss: 1.1748 - val_accuracy: 0.5151\n",
      "Epoch 2/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.1313 - accuracy: 0.5229 - val_loss: 1.1099 - val_accuracy: 0.5271\n",
      "Epoch 3/100\n",
      "609675/609675 [==============================] - 20s 32us/sample - loss: 1.0975 - accuracy: 0.5340 - val_loss: 1.0928 - val_accuracy: 0.5344\n",
      "Epoch 4/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0851 - accuracy: 0.5389 - val_loss: 1.0838 - val_accuracy: 0.5378\n",
      "Epoch 5/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0776 - accuracy: 0.5426 - val_loss: 1.0780 - val_accuracy: 0.5412\n",
      "Epoch 6/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0721 - accuracy: 0.5450 - val_loss: 1.0734 - val_accuracy: 0.5429\n",
      "Epoch 7/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0677 - accuracy: 0.5469 - val_loss: 1.0695 - val_accuracy: 0.5453\n",
      "Epoch 8/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0639 - accuracy: 0.5485 - val_loss: 1.0662 - val_accuracy: 0.5463\n",
      "Epoch 9/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0606 - accuracy: 0.5505 - val_loss: 1.0642 - val_accuracy: 0.5482\n",
      "Epoch 10/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0576 - accuracy: 0.5517 - val_loss: 1.0607 - val_accuracy: 0.5493\n",
      "Epoch 11/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0548 - accuracy: 0.5531 - val_loss: 1.0583 - val_accuracy: 0.5507\n",
      "Epoch 12/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0523 - accuracy: 0.5543 - val_loss: 1.0566 - val_accuracy: 0.5517\n",
      "Epoch 13/100\n",
      "609675/609675 [==============================] - 25s 42us/sample - loss: 1.0499 - accuracy: 0.5558 - val_loss: 1.0544 - val_accuracy: 0.5526\n",
      "Epoch 14/100\n",
      "609675/609675 [==============================] - 26s 43us/sample - loss: 1.0477 - accuracy: 0.5566 - val_loss: 1.0530 - val_accuracy: 0.5531\n",
      "Epoch 15/100\n",
      "609675/609675 [==============================] - 25s 41us/sample - loss: 1.0457 - accuracy: 0.5578 - val_loss: 1.0503 - val_accuracy: 0.5546\n",
      "Epoch 16/100\n",
      "609675/609675 [==============================] - 27s 44us/sample - loss: 1.0437 - accuracy: 0.5590 - val_loss: 1.0486 - val_accuracy: 0.5554\n",
      "Epoch 17/100\n",
      "609675/609675 [==============================] - 25s 41us/sample - loss: 1.0418 - accuracy: 0.5597 - val_loss: 1.0475 - val_accuracy: 0.5559\n",
      "Epoch 18/100\n",
      "609675/609675 [==============================] - 25s 41us/sample - loss: 1.0400 - accuracy: 0.5606 - val_loss: 1.0461 - val_accuracy: 0.5559\n",
      "Epoch 19/100\n",
      "609675/609675 [==============================] - 23s 38us/sample - loss: 1.0383 - accuracy: 0.5613 - val_loss: 1.0446 - val_accuracy: 0.5568\n",
      "Epoch 20/100\n",
      "609675/609675 [==============================] - 23s 37us/sample - loss: 1.0366 - accuracy: 0.5620 - val_loss: 1.0432 - val_accuracy: 0.5576\n",
      "Epoch 21/100\n",
      "609675/609675 [==============================] - 22s 36us/sample - loss: 1.0349 - accuracy: 0.5624 - val_loss: 1.0415 - val_accuracy: 0.5577\n",
      "Epoch 22/100\n",
      "609675/609675 [==============================] - 22s 36us/sample - loss: 1.0334 - accuracy: 0.5631 - val_loss: 1.0402 - val_accuracy: 0.5585\n",
      "Epoch 23/100\n",
      "609675/609675 [==============================] - 22s 36us/sample - loss: 1.0320 - accuracy: 0.5637 - val_loss: 1.0392 - val_accuracy: 0.5589\n",
      "Epoch 24/100\n",
      "609675/609675 [==============================] - 24s 39us/sample - loss: 1.0307 - accuracy: 0.5643 - val_loss: 1.0386 - val_accuracy: 0.5584\n",
      "Epoch 25/100\n",
      "609675/609675 [==============================] - 22s 36us/sample - loss: 1.0293 - accuracy: 0.5650 - val_loss: 1.0373 - val_accuracy: 0.5596\n",
      "Epoch 26/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0281 - accuracy: 0.5653 - val_loss: 1.0367 - val_accuracy: 0.5595\n",
      "Epoch 27/100\n",
      "609675/609675 [==============================] - 23s 37us/sample - loss: 1.0269 - accuracy: 0.5654 - val_loss: 1.0353 - val_accuracy: 0.5604\n",
      "Epoch 28/100\n",
      "609675/609675 [==============================] - 24s 39us/sample - loss: 1.0257 - accuracy: 0.5663 - val_loss: 1.0347 - val_accuracy: 0.5612\n",
      "Epoch 29/100\n",
      "609675/609675 [==============================] - 24s 39us/sample - loss: 1.0247 - accuracy: 0.5663 - val_loss: 1.0343 - val_accuracy: 0.5608\n",
      "Epoch 30/100\n",
      "609675/609675 [==============================] - 24s 39us/sample - loss: 1.0236 - accuracy: 0.5670 - val_loss: 1.0330 - val_accuracy: 0.5614\n",
      "Epoch 31/100\n",
      "609675/609675 [==============================] - 23s 38us/sample - loss: 1.0224 - accuracy: 0.5673 - val_loss: 1.0328 - val_accuracy: 0.5607\n",
      "Epoch 32/100\n",
      "609675/609675 [==============================] - 24s 39us/sample - loss: 1.0214 - accuracy: 0.5678 - val_loss: 1.0322 - val_accuracy: 0.5615\n",
      "Epoch 33/100\n",
      "609675/609675 [==============================] - 24s 39us/sample - loss: 1.0204 - accuracy: 0.5685 - val_loss: 1.0311 - val_accuracy: 0.5620\n",
      "Epoch 34/100\n",
      "609675/609675 [==============================] - 21s 35us/sample - loss: 1.0194 - accuracy: 0.5690 - val_loss: 1.0308 - val_accuracy: 0.5621\n",
      "Epoch 35/100\n",
      "609675/609675 [==============================] - 22s 35us/sample - loss: 1.0184 - accuracy: 0.5693 - val_loss: 1.0302 - val_accuracy: 0.5625\n",
      "Epoch 36/100\n",
      "609675/609675 [==============================] - 21s 34us/sample - loss: 1.0175 - accuracy: 0.5695 - val_loss: 1.0305 - val_accuracy: 0.5625\n",
      "Epoch 37/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0166 - accuracy: 0.5698 - val_loss: 1.0293 - val_accuracy: 0.5627\n",
      "Epoch 38/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0157 - accuracy: 0.5702 - val_loss: 1.0299 - val_accuracy: 0.5620\n",
      "Epoch 39/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0148 - accuracy: 0.5704 - val_loss: 1.0281 - val_accuracy: 0.5633\n",
      "Epoch 40/100\n",
      "609675/609675 [==============================] - 23s 38us/sample - loss: 1.0139 - accuracy: 0.5712 - val_loss: 1.0275 - val_accuracy: 0.5631\n",
      "Epoch 41/100\n",
      "609675/609675 [==============================] - 22s 35us/sample - loss: 1.0130 - accuracy: 0.5710 - val_loss: 1.0272 - val_accuracy: 0.5634\n",
      "Epoch 42/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0122 - accuracy: 0.5711 - val_loss: 1.0269 - val_accuracy: 0.5635\n",
      "Epoch 43/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0114 - accuracy: 0.5719 - val_loss: 1.0273 - val_accuracy: 0.5632\n",
      "Epoch 44/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0106 - accuracy: 0.5722 - val_loss: 1.0261 - val_accuracy: 0.5637\n",
      "Epoch 45/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0097 - accuracy: 0.5725 - val_loss: 1.0260 - val_accuracy: 0.5639\n",
      "Epoch 46/100\n",
      "609675/609675 [==============================] - 21s 34us/sample - loss: 1.0090 - accuracy: 0.5729 - val_loss: 1.0252 - val_accuracy: 0.5642\n",
      "Epoch 47/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0082 - accuracy: 0.5731 - val_loss: 1.0255 - val_accuracy: 0.5641\n",
      "Epoch 48/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0073 - accuracy: 0.5735 - val_loss: 1.0241 - val_accuracy: 0.5646\n",
      "Epoch 49/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0065 - accuracy: 0.5739 - val_loss: 1.0238 - val_accuracy: 0.5643\n",
      "Epoch 50/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0056 - accuracy: 0.5742 - val_loss: 1.0267 - val_accuracy: 0.5633\n",
      "Epoch 51/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0049 - accuracy: 0.5745 - val_loss: 1.0236 - val_accuracy: 0.5647\n",
      "Epoch 52/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0042 - accuracy: 0.5746 - val_loss: 1.0233 - val_accuracy: 0.5648\n",
      "Epoch 53/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0034 - accuracy: 0.5751 - val_loss: 1.0235 - val_accuracy: 0.5651\n",
      "Epoch 54/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0026 - accuracy: 0.5758 - val_loss: 1.0239 - val_accuracy: 0.5649\n",
      "Epoch 55/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0019 - accuracy: 0.5756 - val_loss: 1.0225 - val_accuracy: 0.5659\n",
      "Epoch 56/100\n",
      "609675/609675 [==============================] - 20s 32us/sample - loss: 1.0011 - accuracy: 0.5765 - val_loss: 1.0282 - val_accuracy: 0.5631\n",
      "Epoch 57/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 1.0004 - accuracy: 0.5763 - val_loss: 1.0218 - val_accuracy: 0.5660\n",
      "Epoch 58/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9996 - accuracy: 0.5768 - val_loss: 1.0220 - val_accuracy: 0.5657\n",
      "Epoch 59/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9989 - accuracy: 0.5773 - val_loss: 1.0213 - val_accuracy: 0.5661\n",
      "Epoch 60/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9982 - accuracy: 0.5776 - val_loss: 1.0225 - val_accuracy: 0.5657\n",
      "Epoch 61/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9975 - accuracy: 0.5778 - val_loss: 1.0209 - val_accuracy: 0.5669\n",
      "Epoch 62/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9968 - accuracy: 0.5781 - val_loss: 1.0227 - val_accuracy: 0.5659\n",
      "Epoch 63/100\n",
      "609675/609675 [==============================] - 20s 32us/sample - loss: 0.9961 - accuracy: 0.5784 - val_loss: 1.0214 - val_accuracy: 0.5663\n",
      "Epoch 64/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9954 - accuracy: 0.5786 - val_loss: 1.0197 - val_accuracy: 0.5672\n",
      "Epoch 65/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9948 - accuracy: 0.5787 - val_loss: 1.0196 - val_accuracy: 0.5675\n",
      "Epoch 66/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9940 - accuracy: 0.5791 - val_loss: 1.0211 - val_accuracy: 0.5677\n",
      "Epoch 67/100\n",
      "609675/609675 [==============================] - 20s 34us/sample - loss: 0.9933 - accuracy: 0.5792 - val_loss: 1.0208 - val_accuracy: 0.5671\n",
      "Epoch 68/100\n",
      "609675/609675 [==============================] - 20s 34us/sample - loss: 0.9927 - accuracy: 0.5799 - val_loss: 1.0195 - val_accuracy: 0.5673\n",
      "Epoch 69/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9919 - accuracy: 0.5799 - val_loss: 1.0213 - val_accuracy: 0.5674\n",
      "Epoch 70/100\n",
      "609675/609675 [==============================] - 21s 35us/sample - loss: 0.9913 - accuracy: 0.5805 - val_loss: 1.0190 - val_accuracy: 0.5677\n",
      "Epoch 71/100\n",
      "609675/609675 [==============================] - 23s 38us/sample - loss: 0.9906 - accuracy: 0.5806 - val_loss: 1.0203 - val_accuracy: 0.5677\n",
      "Epoch 72/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9900 - accuracy: 0.5810 - val_loss: 1.0196 - val_accuracy: 0.5678\n",
      "Epoch 73/100\n",
      "609675/609675 [==============================] - 23s 38us/sample - loss: 0.9894 - accuracy: 0.5811 - val_loss: 1.0200 - val_accuracy: 0.5675\n",
      "Epoch 74/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9887 - accuracy: 0.5815 - val_loss: 1.0194 - val_accuracy: 0.5674\n",
      "Epoch 75/100\n",
      "609675/609675 [==============================] - 21s 35us/sample - loss: 0.9881 - accuracy: 0.5812 - val_loss: 1.0190 - val_accuracy: 0.5680\n",
      "Epoch 76/100\n",
      "609675/609675 [==============================] - 22s 36us/sample - loss: 0.9875 - accuracy: 0.5820 - val_loss: 1.0187 - val_accuracy: 0.5688\n",
      "Epoch 77/100\n",
      "609675/609675 [==============================] - 21s 34us/sample - loss: 0.9868 - accuracy: 0.5820 - val_loss: 1.0191 - val_accuracy: 0.5682\n",
      "Epoch 78/100\n",
      "609675/609675 [==============================] - 21s 34us/sample - loss: 0.9862 - accuracy: 0.5826 - val_loss: 1.0192 - val_accuracy: 0.5683\n",
      "Epoch 79/100\n",
      "609675/609675 [==============================] - 20s 33us/sample - loss: 0.9856 - accuracy: 0.5828 - val_loss: 1.0200 - val_accuracy: 0.5683\n",
      "Epoch 80/100\n",
      "609675/609675 [==============================] - 22s 36us/sample - loss: 0.9850 - accuracy: 0.5830 - val_loss: 1.0177 - val_accuracy: 0.5687\n",
      "Epoch 81/100\n",
      "609675/609675 [==============================] - 21s 35us/sample - loss: 0.9844 - accuracy: 0.5831 - val_loss: 1.0203 - val_accuracy: 0.5674\n",
      "Epoch 82/100\n",
      "609675/609675 [==============================] - 22s 36us/sample - loss: 0.9837 - accuracy: 0.5834 - val_loss: 1.0187 - val_accuracy: 0.5690\n",
      "Epoch 83/100\n",
      "609675/609675 [==============================] - 22s 37us/sample - loss: 0.9832 - accuracy: 0.5841 - val_loss: 1.0177 - val_accuracy: 0.5684\n",
      "Epoch 84/100\n",
      "609675/609675 [==============================] - 25s 42us/sample - loss: 0.9825 - accuracy: 0.5840 - val_loss: 1.0188 - val_accuracy: 0.5692\n",
      "Epoch 85/100\n",
      "609675/609675 [==============================] - 24s 40us/sample - loss: 0.9820 - accuracy: 0.5843 - val_loss: 1.0176 - val_accuracy: 0.5690\n",
      "Epoch 86/100\n",
      "609675/609675 [==============================] - 23s 37us/sample - loss: 0.9814 - accuracy: 0.5847 - val_loss: 1.0177 - val_accuracy: 0.5693\n",
      "Epoch 87/100\n",
      "609675/609675 [==============================] - 23s 38us/sample - loss: 0.9808 - accuracy: 0.5848 - val_loss: 1.0177 - val_accuracy: 0.5689\n",
      "Epoch 88/100\n",
      "609675/609675 [==============================] - 22s 35us/sample - loss: 0.9801 - accuracy: 0.5853 - val_loss: 1.0176 - val_accuracy: 0.5697\n",
      "Epoch 89/100\n",
      "609675/609675 [==============================] - 21s 35us/sample - loss: 0.9796 - accuracy: 0.5856 - val_loss: 1.0179 - val_accuracy: 0.5691\n",
      "Epoch 90/100\n",
      "609675/609675 [==============================] - 21s 35us/sample - loss: 0.9791 - accuracy: 0.5858 - val_loss: 1.0180 - val_accuracy: 0.5686\n",
      "Epoch 91/100\n",
      "609675/609675 [==============================] - 24s 39us/sample - loss: 0.9784 - accuracy: 0.5859 - val_loss: 1.0173 - val_accuracy: 0.5697\n",
      "Epoch 92/100\n",
      "609675/609675 [==============================] - 26s 43us/sample - loss: 0.9779 - accuracy: 0.5864 - val_loss: 1.0180 - val_accuracy: 0.5688\n",
      "Epoch 93/100\n",
      "609675/609675 [==============================] - 24s 39us/sample - loss: 0.9774 - accuracy: 0.5865 - val_loss: 1.0181 - val_accuracy: 0.5692\n",
      "Epoch 94/100\n",
      "609675/609675 [==============================] - 21s 35us/sample - loss: 0.9768 - accuracy: 0.5866 - val_loss: 1.0167 - val_accuracy: 0.5701\n",
      "Epoch 95/100\n",
      "609675/609675 [==============================] - 21s 35us/sample - loss: 0.9761 - accuracy: 0.5865 - val_loss: 1.0180 - val_accuracy: 0.5700\n",
      "Epoch 96/100\n",
      "609675/609675 [==============================] - 24s 40us/sample - loss: 0.9757 - accuracy: 0.5871 - val_loss: 1.0188 - val_accuracy: 0.5688\n",
      "Epoch 97/100\n",
      "609675/609675 [==============================] - 26s 42us/sample - loss: 0.9752 - accuracy: 0.5870 - val_loss: 1.0168 - val_accuracy: 0.5696\n",
      "Epoch 98/100\n",
      "609675/609675 [==============================] - 26s 42us/sample - loss: 0.9747 - accuracy: 0.5877 - val_loss: 1.0167 - val_accuracy: 0.5697\n",
      "Epoch 99/100\n",
      "609675/609675 [==============================] - 23s 37us/sample - loss: 0.9741 - accuracy: 0.5874 - val_loss: 1.0177 - val_accuracy: 0.5708\n",
      "Epoch 100/100\n",
      "609675/609675 [==============================] - 28s 45us/sample - loss: 0.9735 - accuracy: 0.5880 - val_loss: 1.0182 - val_accuracy: 0.5693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21884374a88>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.001\n",
    "epochs=100\n",
    "batch_size=100\n",
    "\n",
    "model2 = create_nn(lr)\n",
    "\n",
    "model2.fit(x_train_norm, y_train, \n",
    "           epochs=epochs, batch_size=batch_size, \n",
    "           validation_data=(x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "weighted-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(1000, input_shape=(80,), activation='relu'))\n",
    "    model.add(keras.layers.Dense(500, activation='relu'))\n",
    "    model.add(keras.layers.Dense(200, activation='relu'))\n",
    "    model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(20, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    #stochastic gradient descent\n",
    "    sgd = keras.optimizers.SGD(lr=lr)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "characteristic-student",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 609675 samples, validate on 152419 samples\n",
      "Epoch 1/100\n",
      "609675/609675 [==============================] - 56s 92us/sample - loss: 1.3309 - accuracy: 0.4114 - val_loss: 1.2181 - val_accuracy: 0.4482\n",
      "Epoch 2/100\n",
      "609675/609675 [==============================] - 57s 93us/sample - loss: 1.1635 - accuracy: 0.5022 - val_loss: 1.1285 - val_accuracy: 0.5298\n",
      "Epoch 3/100\n",
      "609675/609675 [==============================] - 58s 95us/sample - loss: 1.1054 - accuracy: 0.5398 - val_loss: 1.0933 - val_accuracy: 0.5434\n",
      "Epoch 4/100\n",
      "609675/609675 [==============================] - 60s 98us/sample - loss: 1.0823 - accuracy: 0.5470 - val_loss: 1.0787 - val_accuracy: 0.5481\n",
      "Epoch 5/100\n",
      "609675/609675 [==============================] - 60s 98us/sample - loss: 1.0710 - accuracy: 0.5510 - val_loss: 1.0706 - val_accuracy: 0.5503\n",
      "Epoch 6/100\n",
      "609675/609675 [==============================] - 60s 99us/sample - loss: 1.0630 - accuracy: 0.5538 - val_loss: 1.0644 - val_accuracy: 0.5530\n",
      "Epoch 7/100\n",
      "609675/609675 [==============================] - 57s 93us/sample - loss: 1.0566 - accuracy: 0.5556 - val_loss: 1.0585 - val_accuracy: 0.5536\n",
      "Epoch 8/100\n",
      "609675/609675 [==============================] - 57s 93us/sample - loss: 1.0511 - accuracy: 0.5577 - val_loss: 1.0549 - val_accuracy: 0.5538\n",
      "Epoch 9/100\n",
      "609675/609675 [==============================] - 64s 105us/sample - loss: 1.0465 - accuracy: 0.5586 - val_loss: 1.0503 - val_accuracy: 0.5554\n",
      "Epoch 10/100\n",
      "609675/609675 [==============================] - 64s 105us/sample - loss: 1.0425 - accuracy: 0.5601 - val_loss: 1.0469 - val_accuracy: 0.5570\n",
      "Epoch 11/100\n",
      "609675/609675 [==============================] - 63s 103us/sample - loss: 1.0390 - accuracy: 0.5612 - val_loss: 1.0453 - val_accuracy: 0.5567\n",
      "Epoch 12/100\n",
      "609675/609675 [==============================] - 57s 93us/sample - loss: 1.0359 - accuracy: 0.5622 - val_loss: 1.0416 - val_accuracy: 0.5582\n",
      "Epoch 13/100\n",
      "609675/609675 [==============================] - 59s 97us/sample - loss: 1.0331 - accuracy: 0.5633 - val_loss: 1.0407 - val_accuracy: 0.5579\n",
      "Epoch 14/100\n",
      "609675/609675 [==============================] - 56s 93us/sample - loss: 1.0305 - accuracy: 0.5641 - val_loss: 1.0374 - val_accuracy: 0.5592\n",
      "Epoch 15/100\n",
      "609675/609675 [==============================] - 59s 97us/sample - loss: 1.0280 - accuracy: 0.5651 - val_loss: 1.0360 - val_accuracy: 0.5592\n",
      "Epoch 16/100\n",
      "609675/609675 [==============================] - 64s 104us/sample - loss: 1.0258 - accuracy: 0.5660 - val_loss: 1.0341 - val_accuracy: 0.5601\n",
      "Epoch 17/100\n",
      "609675/609675 [==============================] - 58s 94us/sample - loss: 1.0237 - accuracy: 0.5663 - val_loss: 1.0332 - val_accuracy: 0.5610\n",
      "Epoch 18/100\n",
      "609675/609675 [==============================] - 56s 91us/sample - loss: 1.0217 - accuracy: 0.5677 - val_loss: 1.0314 - val_accuracy: 0.5617\n",
      "Epoch 19/100\n",
      "609675/609675 [==============================] - 57s 93us/sample - loss: 1.0198 - accuracy: 0.5682 - val_loss: 1.0322 - val_accuracy: 0.5627\n",
      "Epoch 20/100\n",
      "609675/609675 [==============================] - 55s 90us/sample - loss: 1.0180 - accuracy: 0.5691 - val_loss: 1.0315 - val_accuracy: 0.5613\n",
      "Epoch 21/100\n",
      "609675/609675 [==============================] - 57s 93us/sample - loss: 1.0161 - accuracy: 0.5697 - val_loss: 1.0286 - val_accuracy: 0.5635\n",
      "Epoch 22/100\n",
      "609675/609675 [==============================] - 56s 92us/sample - loss: 1.0144 - accuracy: 0.5706 - val_loss: 1.0273 - val_accuracy: 0.5635\n",
      "Epoch 23/100\n",
      "609675/609675 [==============================] - 62s 102us/sample - loss: 1.0127 - accuracy: 0.5711 - val_loss: 1.0263 - val_accuracy: 0.5630\n",
      "Epoch 24/100\n",
      "609675/609675 [==============================] - 72s 119us/sample - loss: 1.0110 - accuracy: 0.5717 - val_loss: 1.0252 - val_accuracy: 0.5649\n",
      "Epoch 25/100\n",
      "609675/609675 [==============================] - 60s 98us/sample - loss: 1.0094 - accuracy: 0.5724 - val_loss: 1.0256 - val_accuracy: 0.5644\n",
      "Epoch 26/100\n",
      "609675/609675 [==============================] - 60s 99us/sample - loss: 1.0079 - accuracy: 0.5732 - val_loss: 1.0339 - val_accuracy: 0.5627\n",
      "Epoch 27/100\n",
      "609675/609675 [==============================] - 67s 111us/sample - loss: 1.0062 - accuracy: 0.5737 - val_loss: 1.0233 - val_accuracy: 0.5655\n",
      "Epoch 28/100\n",
      "609675/609675 [==============================] - 63s 104us/sample - loss: 1.0047 - accuracy: 0.5746 - val_loss: 1.0224 - val_accuracy: 0.5654\n",
      "Epoch 29/100\n",
      "609675/609675 [==============================] - 60s 99us/sample - loss: 1.0032 - accuracy: 0.5752 - val_loss: 1.0224 - val_accuracy: 0.5658\n",
      "Epoch 30/100\n",
      "609675/609675 [==============================] - 64s 106us/sample - loss: 1.0017 - accuracy: 0.5758 - val_loss: 1.0218 - val_accuracy: 0.5663\n",
      "Epoch 31/100\n",
      "609675/609675 [==============================] - 62s 101us/sample - loss: 1.0004 - accuracy: 0.5763 - val_loss: 1.0209 - val_accuracy: 0.5665\n",
      "Epoch 32/100\n",
      "609675/609675 [==============================] - 59s 97us/sample - loss: 0.9989 - accuracy: 0.5770 - val_loss: 1.0204 - val_accuracy: 0.5671\n",
      "Epoch 33/100\n",
      "609675/609675 [==============================] - 58s 95us/sample - loss: 0.9976 - accuracy: 0.5775 - val_loss: 1.0222 - val_accuracy: 0.5665\n",
      "Epoch 34/100\n",
      "609675/609675 [==============================] - 58s 95us/sample - loss: 0.9960 - accuracy: 0.5778 - val_loss: 1.0323 - val_accuracy: 0.5640\n",
      "Epoch 35/100\n",
      "609675/609675 [==============================] - 59s 97us/sample - loss: 0.9946 - accuracy: 0.5788 - val_loss: 1.0187 - val_accuracy: 0.5681\n",
      "Epoch 36/100\n",
      "609675/609675 [==============================] - 64s 105us/sample - loss: 0.9934 - accuracy: 0.5792 - val_loss: 1.0181 - val_accuracy: 0.5677\n",
      "Epoch 37/100\n",
      "609675/609675 [==============================] - 68s 112us/sample - loss: 0.9920 - accuracy: 0.5799 - val_loss: 1.0181 - val_accuracy: 0.5682\n",
      "Epoch 38/100\n",
      "609675/609675 [==============================] - 61s 99us/sample - loss: 0.9909 - accuracy: 0.5803 - val_loss: 1.0180 - val_accuracy: 0.5686\n",
      "Epoch 39/100\n",
      "609675/609675 [==============================] - 59s 97us/sample - loss: 0.9895 - accuracy: 0.5810 - val_loss: 1.0189 - val_accuracy: 0.5680\n",
      "Epoch 40/100\n",
      "609675/609675 [==============================] - 62s 102us/sample - loss: 0.9882 - accuracy: 0.5813 - val_loss: 1.0210 - val_accuracy: 0.5681\n",
      "Epoch 41/100\n",
      "609675/609675 [==============================] - 58s 96us/sample - loss: 0.9871 - accuracy: 0.5821 - val_loss: 1.0175 - val_accuracy: 0.5681\n",
      "Epoch 42/100\n",
      "609675/609675 [==============================] - 61s 100us/sample - loss: 0.9857 - accuracy: 0.5827 - val_loss: 1.0203 - val_accuracy: 0.5683\n",
      "Epoch 43/100\n",
      "609675/609675 [==============================] - 62s 102us/sample - loss: 0.9844 - accuracy: 0.5832 - val_loss: 1.0266 - val_accuracy: 0.5658\n",
      "Epoch 44/100\n",
      "609675/609675 [==============================] - 66s 107us/sample - loss: 0.9832 - accuracy: 0.5838 - val_loss: 1.0181 - val_accuracy: 0.5689\n",
      "Epoch 45/100\n",
      "609675/609675 [==============================] - 60s 98us/sample - loss: 0.9821 - accuracy: 0.5843 - val_loss: 1.0251 - val_accuracy: 0.5673\n",
      "Epoch 46/100\n",
      "609675/609675 [==============================] - 62s 102us/sample - loss: 0.9809 - accuracy: 0.5847 - val_loss: 1.0173 - val_accuracy: 0.5696\n",
      "Epoch 47/100\n",
      "609675/609675 [==============================] - 63s 104us/sample - loss: 0.9795 - accuracy: 0.5852 - val_loss: 1.0169 - val_accuracy: 0.5705\n",
      "Epoch 48/100\n",
      "609675/609675 [==============================] - 64s 105us/sample - loss: 0.9786 - accuracy: 0.5857 - val_loss: 1.0168 - val_accuracy: 0.5689\n",
      "Epoch 49/100\n",
      "609675/609675 [==============================] - 62s 102us/sample - loss: 0.9776 - accuracy: 0.5861 - val_loss: 1.0166 - val_accuracy: 0.5693\n",
      "Epoch 50/100\n",
      "609675/609675 [==============================] - 58s 95us/sample - loss: 0.9760 - accuracy: 0.5864 - val_loss: 1.0162 - val_accuracy: 0.5706\n",
      "Epoch 51/100\n",
      "609675/609675 [==============================] - 59s 96us/sample - loss: 0.9752 - accuracy: 0.5870 - val_loss: 1.0181 - val_accuracy: 0.5691\n",
      "Epoch 52/100\n",
      "609675/609675 [==============================] - 60s 99us/sample - loss: 0.9739 - accuracy: 0.5874 - val_loss: 1.0160 - val_accuracy: 0.5701\n",
      "Epoch 53/100\n",
      "609675/609675 [==============================] - 61s 100us/sample - loss: 0.9729 - accuracy: 0.5881 - val_loss: 1.0156 - val_accuracy: 0.5706\n",
      "Epoch 54/100\n",
      "609675/609675 [==============================] - 59s 98us/sample - loss: 0.9719 - accuracy: 0.5884 - val_loss: 1.0290 - val_accuracy: 0.5650\n",
      "Epoch 55/100\n",
      "609675/609675 [==============================] - 61s 100us/sample - loss: 0.9705 - accuracy: 0.5888 - val_loss: 1.0178 - val_accuracy: 0.5692\n",
      "Epoch 56/100\n",
      "609675/609675 [==============================] - 60s 99us/sample - loss: 0.9696 - accuracy: 0.5891 - val_loss: 1.0204 - val_accuracy: 0.5696\n",
      "Epoch 57/100\n",
      "609675/609675 [==============================] - 62s 102us/sample - loss: 0.9685 - accuracy: 0.5896 - val_loss: 1.0173 - val_accuracy: 0.5698\n",
      "Epoch 58/100\n",
      "609675/609675 [==============================] - 67s 109us/sample - loss: 0.9673 - accuracy: 0.5900 - val_loss: 1.0200 - val_accuracy: 0.5698\n",
      "Epoch 59/100\n",
      "609675/609675 [==============================] - 60s 99us/sample - loss: 0.9663 - accuracy: 0.5911 - val_loss: 1.0160 - val_accuracy: 0.5707\n",
      "Epoch 60/100\n",
      "609675/609675 [==============================] - 67s 110us/sample - loss: 0.9651 - accuracy: 0.5915 - val_loss: 1.0176 - val_accuracy: 0.5709\n",
      "Epoch 61/100\n",
      "609675/609675 [==============================] - 68s 112us/sample - loss: 0.9642 - accuracy: 0.5919 - val_loss: 1.0221 - val_accuracy: 0.5680\n",
      "Epoch 62/100\n",
      "609675/609675 [==============================] - 66s 108us/sample - loss: 0.9630 - accuracy: 0.5925 - val_loss: 1.0178 - val_accuracy: 0.5709\n",
      "Epoch 63/100\n",
      "609675/609675 [==============================] - 68s 112us/sample - loss: 0.9620 - accuracy: 0.5924 - val_loss: 1.0182 - val_accuracy: 0.5713\n",
      "Epoch 64/100\n",
      "609675/609675 [==============================] - 66s 108us/sample - loss: 0.9608 - accuracy: 0.5933 - val_loss: 1.0173 - val_accuracy: 0.5709\n",
      "Epoch 65/100\n",
      "609675/609675 [==============================] - 67s 110us/sample - loss: 0.9601 - accuracy: 0.5934 - val_loss: 1.0198 - val_accuracy: 0.5701\n",
      "Epoch 66/100\n",
      "609675/609675 [==============================] - 64s 105us/sample - loss: 0.9588 - accuracy: 0.5944 - val_loss: 1.0226 - val_accuracy: 0.5695\n",
      "Epoch 67/100\n",
      "609675/609675 [==============================] - 72s 118us/sample - loss: 0.9579 - accuracy: 0.5945 - val_loss: 1.0177 - val_accuracy: 0.5701\n",
      "Epoch 68/100\n",
      "609675/609675 [==============================] - 64s 105us/sample - loss: 0.9568 - accuracy: 0.5950 - val_loss: 1.0175 - val_accuracy: 0.5719\n",
      "Epoch 69/100\n",
      "609675/609675 [==============================] - 62s 101us/sample - loss: 0.9558 - accuracy: 0.5954 - val_loss: 1.0176 - val_accuracy: 0.5709\n",
      "Epoch 70/100\n",
      "609675/609675 [==============================] - 61s 99us/sample - loss: 0.9548 - accuracy: 0.5957 - val_loss: 1.0187 - val_accuracy: 0.5716\n",
      "Epoch 71/100\n",
      "609675/609675 [==============================] - 60s 98us/sample - loss: 0.9538 - accuracy: 0.5964 - val_loss: 1.0238 - val_accuracy: 0.5701\n",
      "Epoch 72/100\n",
      "609675/609675 [==============================] - 61s 101us/sample - loss: 0.9527 - accuracy: 0.5967 - val_loss: 1.0186 - val_accuracy: 0.5709\n",
      "Epoch 73/100\n",
      "609675/609675 [==============================] - 62s 102us/sample - loss: 0.9517 - accuracy: 0.5969 - val_loss: 1.0454 - val_accuracy: 0.5654\n",
      "Epoch 74/100\n",
      "609675/609675 [==============================] - 71s 117us/sample - loss: 0.9509 - accuracy: 0.5976 - val_loss: 1.0219 - val_accuracy: 0.5711\n",
      "Epoch 75/100\n",
      "609675/609675 [==============================] - 63s 103us/sample - loss: 0.9494 - accuracy: 0.5981 - val_loss: 1.0222 - val_accuracy: 0.5702\n",
      "Epoch 76/100\n",
      "609675/609675 [==============================] - 63s 103us/sample - loss: 0.9487 - accuracy: 0.5982 - val_loss: 1.0161 - val_accuracy: 0.5720\n",
      "Epoch 77/100\n",
      "609675/609675 [==============================] - 61s 100us/sample - loss: 0.9478 - accuracy: 0.5990 - val_loss: 1.0204 - val_accuracy: 0.5705\n",
      "Epoch 78/100\n",
      "609675/609675 [==============================] - 61s 100us/sample - loss: 0.9466 - accuracy: 0.5992 - val_loss: 1.0234 - val_accuracy: 0.5704\n",
      "Epoch 79/100\n",
      "609675/609675 [==============================] - 60s 98us/sample - loss: 0.9456 - accuracy: 0.6000 - val_loss: 1.0275 - val_accuracy: 0.5691\n",
      "Epoch 80/100\n",
      "609675/609675 [==============================] - 61s 100us/sample - loss: 0.9450 - accuracy: 0.6000 - val_loss: 1.0230 - val_accuracy: 0.5686\n",
      "Epoch 81/100\n",
      "609675/609675 [==============================] - 60s 98us/sample - loss: 0.9434 - accuracy: 0.6005 - val_loss: 1.0237 - val_accuracy: 0.5703\n",
      "Epoch 82/100\n",
      "609675/609675 [==============================] - 61s 100us/sample - loss: 0.9425 - accuracy: 0.6017 - val_loss: 1.0184 - val_accuracy: 0.5714\n",
      "Epoch 83/100\n",
      "609675/609675 [==============================] - 65s 106us/sample - loss: 0.9419 - accuracy: 0.6012 - val_loss: 1.0222 - val_accuracy: 0.5703\n",
      "Epoch 84/100\n",
      "609675/609675 [==============================] - 59s 97us/sample - loss: 0.9409 - accuracy: 0.6024 - val_loss: 1.0218 - val_accuracy: 0.5697\n",
      "Epoch 85/100\n",
      "609675/609675 [==============================] - 66s 109us/sample - loss: 0.9399 - accuracy: 0.6022 - val_loss: 1.0231 - val_accuracy: 0.5707\n",
      "Epoch 86/100\n",
      "609675/609675 [==============================] - 68s 111us/sample - loss: 0.9385 - accuracy: 0.6030 - val_loss: 1.0216 - val_accuracy: 0.5711\n",
      "Epoch 87/100\n",
      "609675/609675 [==============================] - 67s 110us/sample - loss: 0.9382 - accuracy: 0.6034 - val_loss: 1.0208 - val_accuracy: 0.5724\n",
      "Epoch 88/100\n",
      "609675/609675 [==============================] - 64s 106us/sample - loss: 0.9369 - accuracy: 0.6040 - val_loss: 1.0195 - val_accuracy: 0.5709\n",
      "Epoch 89/100\n",
      "609675/609675 [==============================] - 64s 105us/sample - loss: 0.9361 - accuracy: 0.6044 - val_loss: 1.0237 - val_accuracy: 0.5705\n",
      "Epoch 90/100\n",
      "498800/609675 [=======================>......] - ETA: 10s - loss: 0.9344 - accuracy: 0.6042"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-4910a2ec324d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m model3.fit(x_train_norm, y_train, \n\u001b[0;32m      8\u001b[0m            \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m            validation_data=(x_test_norm, y_test))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "epochs=100\n",
    "batch_size=100\n",
    "\n",
    "model3 = create_nn(lr)\n",
    "\n",
    "model3.fit(x_train_norm, y_train, \n",
    "           epochs=epochs, batch_size=batch_size, \n",
    "           validation_data=(x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-loading",
   "metadata": {},
   "source": [
    "It stabilized around .57, which is about the same performance as model 2 (with one less layer) achieved at around epoch 60; but model 2 takes half the time to run for each epoch. Thus model 2 with 60 epochs is chosen. Model 2 has the following structure:\n",
    "   - 7 layers with 500, 200, 100, 50, 20, 10 and 5 units\n",
    "   \n",
    "Now let's learning play with the batch size and the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "missing-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(500, input_shape=(80,), activation='relu'))\n",
    "    model.add(keras.layers.Dense(200, activation='relu'))\n",
    "    model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(20, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    #stochastic gradient descent\n",
    "    sgd = keras.optimizers.SGD(lr=lr)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "white-queue",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 609675 samples, validate on 152419 samples\n",
      "Epoch 1/60\n",
      "609675/609675 [==============================] - 31s 52us/sample - loss: 1.2793 - accuracy: 0.4233 - val_loss: 1.1846 - val_accuracy: 0.4739\n",
      "Epoch 2/60\n",
      "609675/609675 [==============================] - 32s 53us/sample - loss: 1.1509 - accuracy: 0.5087 - val_loss: 1.1318 - val_accuracy: 0.5336\n",
      "Epoch 3/60\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 1.1126 - accuracy: 0.5407 - val_loss: 1.1039 - val_accuracy: 0.5411\n",
      "Epoch 4/60\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 1.0915 - accuracy: 0.5454 - val_loss: 1.0880 - val_accuracy: 0.5452\n",
      "Epoch 5/60\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 1.0783 - accuracy: 0.5491 - val_loss: 1.0777 - val_accuracy: 0.5484\n",
      "Epoch 6/60\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 1.0692 - accuracy: 0.5518 - val_loss: 1.0697 - val_accuracy: 0.5498\n",
      "Epoch 7/60\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 1.0623 - accuracy: 0.5538 - val_loss: 1.0642 - val_accuracy: 0.5515\n",
      "Epoch 8/60\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 1.0569 - accuracy: 0.5552 - val_loss: 1.0597 - val_accuracy: 0.5527\n",
      "Epoch 9/60\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 1.0524 - accuracy: 0.5568 - val_loss: 1.0560 - val_accuracy: 0.5533\n",
      "Epoch 10/60\n",
      "609675/609675 [==============================] - 31s 52us/sample - loss: 1.0485 - accuracy: 0.5581 - val_loss: 1.0530 - val_accuracy: 0.5546\n",
      "Epoch 11/60\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 1.0452 - accuracy: 0.5592 - val_loss: 1.0500 - val_accuracy: 0.5561\n",
      "Epoch 12/60\n",
      "609675/609675 [==============================] - 38s 62us/sample - loss: 1.0422 - accuracy: 0.5600 - val_loss: 1.0488 - val_accuracy: 0.5566\n",
      "Epoch 13/60\n",
      "609675/609675 [==============================] - 43s 71us/sample - loss: 1.0394 - accuracy: 0.5612 - val_loss: 1.0453 - val_accuracy: 0.5575\n",
      "Epoch 14/60\n",
      "609675/609675 [==============================] - 31s 50us/sample - loss: 1.0370 - accuracy: 0.5622 - val_loss: 1.0430 - val_accuracy: 0.5584\n",
      "Epoch 15/60\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 1.0346 - accuracy: 0.5634 - val_loss: 1.0421 - val_accuracy: 0.5591\n",
      "Epoch 16/60\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 1.0323 - accuracy: 0.5645 - val_loss: 1.0398 - val_accuracy: 0.5599\n",
      "Epoch 17/60\n",
      "609675/609675 [==============================] - 31s 50us/sample - loss: 1.0302 - accuracy: 0.5651 - val_loss: 1.0383 - val_accuracy: 0.5598\n",
      "Epoch 18/60\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 1.0282 - accuracy: 0.5658 - val_loss: 1.0385 - val_accuracy: 0.5597\n",
      "Epoch 19/60\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 1.0262 - accuracy: 0.5665 - val_loss: 1.0354 - val_accuracy: 0.5614\n",
      "Epoch 20/60\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 1.0243 - accuracy: 0.5672 - val_loss: 1.0333 - val_accuracy: 0.5619\n",
      "Epoch 21/60\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 1.0225 - accuracy: 0.5682 - val_loss: 1.0331 - val_accuracy: 0.5619\n",
      "Epoch 22/60\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 1.0208 - accuracy: 0.5688 - val_loss: 1.0321 - val_accuracy: 0.5624\n",
      "Epoch 23/60\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 1.0192 - accuracy: 0.5694 - val_loss: 1.0307 - val_accuracy: 0.5631\n",
      "Epoch 24/60\n",
      "609675/609675 [==============================] - 32s 53us/sample - loss: 1.0175 - accuracy: 0.5702 - val_loss: 1.0294 - val_accuracy: 0.5632\n",
      "Epoch 25/60\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 1.0160 - accuracy: 0.5706 - val_loss: 1.0290 - val_accuracy: 0.5633\n",
      "Epoch 26/60\n",
      "609675/609675 [==============================] - 33s 55us/sample - loss: 1.0146 - accuracy: 0.5711 - val_loss: 1.0277 - val_accuracy: 0.5641\n",
      "Epoch 27/60\n",
      "609675/609675 [==============================] - 29s 48us/sample - loss: 1.0131 - accuracy: 0.5717 - val_loss: 1.0269 - val_accuracy: 0.5643\n",
      "Epoch 28/60\n",
      "609675/609675 [==============================] - 29s 47us/sample - loss: 1.0117 - accuracy: 0.5722 - val_loss: 1.0285 - val_accuracy: 0.5635\n",
      "Epoch 29/60\n",
      "609675/609675 [==============================] - 29s 47us/sample - loss: 1.0104 - accuracy: 0.5729 - val_loss: 1.0277 - val_accuracy: 0.5639\n",
      "Epoch 30/60\n",
      "609675/609675 [==============================] - 28s 46us/sample - loss: 1.0092 - accuracy: 0.5731 - val_loss: 1.0276 - val_accuracy: 0.5645\n",
      "Epoch 31/60\n",
      "609675/609675 [==============================] - 28s 46us/sample - loss: 1.0078 - accuracy: 0.5738 - val_loss: 1.0236 - val_accuracy: 0.5660\n",
      "Epoch 32/60\n",
      "609675/609675 [==============================] - 28s 46us/sample - loss: 1.0066 - accuracy: 0.5743 - val_loss: 1.0238 - val_accuracy: 0.5656\n",
      "Epoch 33/60\n",
      "609675/609675 [==============================] - 29s 47us/sample - loss: 1.0054 - accuracy: 0.5747 - val_loss: 1.0232 - val_accuracy: 0.5664\n",
      "Epoch 34/60\n",
      "609675/609675 [==============================] - 28s 46us/sample - loss: 1.0042 - accuracy: 0.5751 - val_loss: 1.0257 - val_accuracy: 0.5643\n",
      "Epoch 35/60\n",
      "609675/609675 [==============================] - 28s 47us/sample - loss: 1.0032 - accuracy: 0.5755 - val_loss: 1.0213 - val_accuracy: 0.5666\n",
      "Epoch 36/60\n",
      "609675/609675 [==============================] - 28s 47us/sample - loss: 1.0020 - accuracy: 0.5761 - val_loss: 1.0213 - val_accuracy: 0.5663\n",
      "Epoch 37/60\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 1.0008 - accuracy: 0.5767 - val_loss: 1.0255 - val_accuracy: 0.5647\n",
      "Epoch 38/60\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 0.9998 - accuracy: 0.5768 - val_loss: 1.0213 - val_accuracy: 0.5666\n",
      "Epoch 39/60\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 0.9987 - accuracy: 0.5774 - val_loss: 1.0197 - val_accuracy: 0.5669\n",
      "Epoch 40/60\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 0.9976 - accuracy: 0.5782 - val_loss: 1.0194 - val_accuracy: 0.5670\n",
      "Epoch 41/60\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 0.9966 - accuracy: 0.5779 - val_loss: 1.0232 - val_accuracy: 0.5663\n",
      "Epoch 42/60\n",
      "609675/609675 [==============================] - 33s 55us/sample - loss: 0.9955 - accuracy: 0.5787 - val_loss: 1.0185 - val_accuracy: 0.5677\n",
      "Epoch 43/60\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 0.9946 - accuracy: 0.5789 - val_loss: 1.0200 - val_accuracy: 0.5672\n",
      "Epoch 44/60\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 0.9936 - accuracy: 0.5795 - val_loss: 1.0245 - val_accuracy: 0.5661\n",
      "Epoch 45/60\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 0.9926 - accuracy: 0.5803 - val_loss: 1.0194 - val_accuracy: 0.5677\n",
      "Epoch 46/60\n",
      "609675/609675 [==============================] - 35s 57us/sample - loss: 0.9916 - accuracy: 0.5803 - val_loss: 1.0218 - val_accuracy: 0.5665\n",
      "Epoch 47/60\n",
      "609675/609675 [==============================] - 29s 48us/sample - loss: 0.9906 - accuracy: 0.5806 - val_loss: 1.0184 - val_accuracy: 0.5681\n",
      "Epoch 48/60\n",
      "609675/609675 [==============================] - 33s 55us/sample - loss: 0.9896 - accuracy: 0.5813 - val_loss: 1.0195 - val_accuracy: 0.5667\n",
      "Epoch 49/60\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9888 - accuracy: 0.5812 - val_loss: 1.0304 - val_accuracy: 0.5630\n",
      "Epoch 50/60\n",
      "609675/609675 [==============================] - 37s 61us/sample - loss: 0.9878 - accuracy: 0.5818 - val_loss: 1.0199 - val_accuracy: 0.5675\n",
      "Epoch 51/60\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 0.9870 - accuracy: 0.5823 - val_loss: 1.0211 - val_accuracy: 0.5666\n",
      "Epoch 52/60\n",
      "609675/609675 [==============================] - 30s 48us/sample - loss: 0.9862 - accuracy: 0.5829 - val_loss: 1.0209 - val_accuracy: 0.5666\n",
      "Epoch 53/60\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 0.9853 - accuracy: 0.5832 - val_loss: 1.0192 - val_accuracy: 0.5679\n",
      "Epoch 54/60\n",
      "609675/609675 [==============================] - 31s 50us/sample - loss: 0.9844 - accuracy: 0.5837 - val_loss: 1.0178 - val_accuracy: 0.5684\n",
      "Epoch 55/60\n",
      "609675/609675 [==============================] - 32s 53us/sample - loss: 0.9835 - accuracy: 0.5839 - val_loss: 1.0219 - val_accuracy: 0.5672\n",
      "Epoch 56/60\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 0.9825 - accuracy: 0.5839 - val_loss: 1.0174 - val_accuracy: 0.5684\n",
      "Epoch 57/60\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 0.9817 - accuracy: 0.5844 - val_loss: 1.0164 - val_accuracy: 0.5692\n",
      "Epoch 58/60\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9808 - accuracy: 0.5849 - val_loss: 1.0168 - val_accuracy: 0.5690\n",
      "Epoch 59/60\n",
      "609675/609675 [==============================] - 28s 47us/sample - loss: 0.9801 - accuracy: 0.5850 - val_loss: 1.0163 - val_accuracy: 0.5697\n",
      "Epoch 60/60\n",
      "609675/609675 [==============================] - 28s 46us/sample - loss: 0.9792 - accuracy: 0.5856 - val_loss: 1.0196 - val_accuracy: 0.5681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21888755788>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.001\n",
    "epochs=60\n",
    "batch_size=60\n",
    "\n",
    "model2_b = create_nn(lr)\n",
    "\n",
    "model2_b.fit(x_train_norm, y_train, \n",
    "           epochs=epochs, batch_size=batch_size,\n",
    "           validation_data=(x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-anxiety",
   "metadata": {},
   "source": [
    "With batch size = 60, it takes 40 epochs to reach .57 validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "national-senator",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 609675 samples, validate on 152419 samples\n",
      "Epoch 1/40\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 1.0815 - accuracy: 0.5401 - val_loss: 1.0545 - val_accuracy: 0.5505\n",
      "Epoch 2/40\n",
      "609675/609675 [==============================] - 32s 53us/sample - loss: 1.0404 - accuracy: 0.5592 - val_loss: 1.0378 - val_accuracy: 0.5574\n",
      "Epoch 3/40\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 1.0290 - accuracy: 0.5635 - val_loss: 1.0319 - val_accuracy: 0.5604\n",
      "Epoch 4/40\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 1.0213 - accuracy: 0.5667 - val_loss: 1.0258 - val_accuracy: 0.5631\n",
      "Epoch 5/40\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 1.0151 - accuracy: 0.5694 - val_loss: 1.0324 - val_accuracy: 0.5612\n",
      "Epoch 6/40\n",
      "609675/609675 [==============================] - 33s 55us/sample - loss: 1.0101 - accuracy: 0.5711 - val_loss: 1.0232 - val_accuracy: 0.5647\n",
      "Epoch 7/40\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 1.0053 - accuracy: 0.5732 - val_loss: 1.0219 - val_accuracy: 0.5642\n",
      "Epoch 8/40\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 1.0013 - accuracy: 0.5751 - val_loss: 1.0199 - val_accuracy: 0.5657\n",
      "Epoch 9/40\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 0.9974 - accuracy: 0.5770 - val_loss: 1.0162 - val_accuracy: 0.5670\n",
      "Epoch 10/40\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 0.9940 - accuracy: 0.5782 - val_loss: 1.0138 - val_accuracy: 0.5679\n",
      "Epoch 11/40\n",
      "609675/609675 [==============================] - 31s 50us/sample - loss: 0.9907 - accuracy: 0.5792 - val_loss: 1.0166 - val_accuracy: 0.5683\n",
      "Epoch 12/40\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 0.9876 - accuracy: 0.5809 - val_loss: 1.0131 - val_accuracy: 0.5688\n",
      "Epoch 13/40\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9850 - accuracy: 0.5820 - val_loss: 1.0299 - val_accuracy: 0.5604\n",
      "Epoch 14/40\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 0.9823 - accuracy: 0.5832 - val_loss: 1.0222 - val_accuracy: 0.5646\n",
      "Epoch 15/40\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 0.9794 - accuracy: 0.5847 - val_loss: 1.0109 - val_accuracy: 0.5704\n",
      "Epoch 16/40\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 0.9769 - accuracy: 0.5857 - val_loss: 1.0146 - val_accuracy: 0.5687\n",
      "Epoch 17/40\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 0.9745 - accuracy: 0.5866 - val_loss: 1.0103 - val_accuracy: 0.5702\n",
      "Epoch 18/40\n",
      "609675/609675 [==============================] - 29s 47us/sample - loss: 0.9719 - accuracy: 0.5879 - val_loss: 1.0143 - val_accuracy: 0.5714\n",
      "Epoch 19/40\n",
      "609675/609675 [==============================] - 29s 48us/sample - loss: 0.9696 - accuracy: 0.5893 - val_loss: 1.0083 - val_accuracy: 0.5710\n",
      "Epoch 20/40\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 0.9674 - accuracy: 0.5902 - val_loss: 1.0093 - val_accuracy: 0.5710\n",
      "Epoch 21/40\n",
      "609675/609675 [==============================] - 34s 56us/sample - loss: 0.9652 - accuracy: 0.5914 - val_loss: 1.0105 - val_accuracy: 0.5711\n",
      "Epoch 22/40\n",
      "609675/609675 [==============================] - 31s 52us/sample - loss: 0.9630 - accuracy: 0.5929 - val_loss: 1.0094 - val_accuracy: 0.5686\n",
      "Epoch 23/40\n",
      "609675/609675 [==============================] - 32s 53us/sample - loss: 0.9608 - accuracy: 0.5930 - val_loss: 1.0055 - val_accuracy: 0.5733\n",
      "Epoch 24/40\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 0.9588 - accuracy: 0.5944 - val_loss: 1.0080 - val_accuracy: 0.5718\n",
      "Epoch 25/40\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 0.9565 - accuracy: 0.5956 - val_loss: 1.0132 - val_accuracy: 0.5711\n",
      "Epoch 26/40\n",
      "609675/609675 [==============================] - 29s 48us/sample - loss: 0.9547 - accuracy: 0.5964 - val_loss: 1.0148 - val_accuracy: 0.5696\n",
      "Epoch 27/40\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 0.9526 - accuracy: 0.5967 - val_loss: 1.0112 - val_accuracy: 0.5705\n",
      "Epoch 28/40\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 0.9507 - accuracy: 0.5980 - val_loss: 1.0138 - val_accuracy: 0.5696\n",
      "Epoch 29/40\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 0.9486 - accuracy: 0.5989 - val_loss: 1.0118 - val_accuracy: 0.5719\n",
      "Epoch 30/40\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9467 - accuracy: 0.5999 - val_loss: 1.0141 - val_accuracy: 0.5724\n",
      "Epoch 31/40\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 0.9450 - accuracy: 0.6014 - val_loss: 1.0130 - val_accuracy: 0.5711\n",
      "Epoch 32/40\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 0.9431 - accuracy: 0.6021 - val_loss: 1.0140 - val_accuracy: 0.5721\n",
      "Epoch 33/40\n",
      "609675/609675 [==============================] - 31s 50us/sample - loss: 0.9415 - accuracy: 0.6023 - val_loss: 1.0294 - val_accuracy: 0.5661\n",
      "Epoch 34/40\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 0.9396 - accuracy: 0.6029 - val_loss: 1.0123 - val_accuracy: 0.5698\n",
      "Epoch 35/40\n",
      "609675/609675 [==============================] - 36s 59us/sample - loss: 0.9381 - accuracy: 0.6036 - val_loss: 1.0126 - val_accuracy: 0.5724\n",
      "Epoch 36/40\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9360 - accuracy: 0.6046 - val_loss: 1.0165 - val_accuracy: 0.5722\n",
      "Epoch 37/40\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 0.9345 - accuracy: 0.6054 - val_loss: 1.0110 - val_accuracy: 0.5706\n",
      "Epoch 38/40\n",
      "609675/609675 [==============================] - 33s 55us/sample - loss: 0.9325 - accuracy: 0.6063 - val_loss: 1.0127 - val_accuracy: 0.5715\n",
      "Epoch 39/40\n",
      "609675/609675 [==============================] - 35s 57us/sample - loss: 0.9309 - accuracy: 0.6072 - val_loss: 1.0149 - val_accuracy: 0.5713\n",
      "Epoch 40/40\n",
      "609675/609675 [==============================] - 32s 53us/sample - loss: 0.9290 - accuracy: 0.6078 - val_loss: 1.0166 - val_accuracy: 0.5727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x218887134c8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.01\n",
    "epochs=40\n",
    "batch_size=60\n",
    "\n",
    "model2_c = create_nn(lr)\n",
    "\n",
    "model2_c.fit(x_train_norm, y_train, \n",
    "           epochs=epochs, batch_size=batch_size, \n",
    "           validation_data=(x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-translation",
   "metadata": {},
   "source": [
    "Decreasing the learning rate has slightly improved the validation error, let's try to decrease it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "developed-lambda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 609675 samples, validate on 152419 samples\n",
      "Epoch 1/40\n",
      "609675/609675 [==============================] - 32s 53us/sample - loss: 1.0587 - accuracy: 0.5502 - val_loss: 1.0538 - val_accuracy: 0.5495\n",
      "Epoch 2/40\n",
      "609675/609675 [==============================] - 35s 57us/sample - loss: 1.0317 - accuracy: 0.5622 - val_loss: 1.0398 - val_accuracy: 0.5573\n",
      "Epoch 3/40\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 1.0206 - accuracy: 0.5664 - val_loss: 1.0273 - val_accuracy: 0.5620\n",
      "Epoch 4/40\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 1.0128 - accuracy: 0.5698 - val_loss: 1.0198 - val_accuracy: 0.5652\n",
      "Epoch 5/40\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 1.0063 - accuracy: 0.5716 - val_loss: 1.0212 - val_accuracy: 0.5648\n",
      "Epoch 6/40\n",
      "609675/609675 [==============================] - 36s 60us/sample - loss: 1.0006 - accuracy: 0.5745 - val_loss: 1.0126 - val_accuracy: 0.5675\n",
      "Epoch 7/40\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 0.9957 - accuracy: 0.5760 - val_loss: 1.0198 - val_accuracy: 0.5651\n",
      "Epoch 8/40\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9911 - accuracy: 0.5789 - val_loss: 1.0120 - val_accuracy: 0.5697\n",
      "Epoch 9/40\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 0.9869 - accuracy: 0.5803 - val_loss: 1.0157 - val_accuracy: 0.5668\n",
      "Epoch 10/40\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 0.9828 - accuracy: 0.5825 - val_loss: 1.0079 - val_accuracy: 0.5713\n",
      "Epoch 11/40\n",
      "609675/609675 [==============================] - 34s 56us/sample - loss: 0.9791 - accuracy: 0.5840 - val_loss: 1.0061 - val_accuracy: 0.5730\n",
      "Epoch 12/40\n",
      "609675/609675 [==============================] - 36s 59us/sample - loss: 0.9753 - accuracy: 0.5855 - val_loss: 1.0044 - val_accuracy: 0.5720\n",
      "Epoch 13/40\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 0.9719 - accuracy: 0.5869 - val_loss: 1.0047 - val_accuracy: 0.5730\n",
      "Epoch 14/40\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9686 - accuracy: 0.5881 - val_loss: 1.0114 - val_accuracy: 0.5702\n",
      "Epoch 15/40\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 0.9656 - accuracy: 0.5900 - val_loss: 1.0016 - val_accuracy: 0.5753\n",
      "Epoch 16/40\n",
      "609675/609675 [==============================] - 33s 55us/sample - loss: 0.9629 - accuracy: 0.5911 - val_loss: 1.0038 - val_accuracy: 0.5724\n",
      "Epoch 17/40\n",
      "609675/609675 [==============================] - 37s 61us/sample - loss: 0.9596 - accuracy: 0.5927 - val_loss: 1.0054 - val_accuracy: 0.5756\n",
      "Epoch 18/40\n",
      "609675/609675 [==============================] - 34s 56us/sample - loss: 0.9567 - accuracy: 0.5941 - val_loss: 1.0087 - val_accuracy: 0.5709\n",
      "Epoch 19/40\n",
      "609675/609675 [==============================] - 41s 67us/sample - loss: 0.9545 - accuracy: 0.5943 - val_loss: 1.0066 - val_accuracy: 0.5746\n",
      "Epoch 20/40\n",
      "609675/609675 [==============================] - 37s 61us/sample - loss: 0.9513 - accuracy: 0.5964 - val_loss: 1.0084 - val_accuracy: 0.5734\n",
      "Epoch 21/40\n",
      "609675/609675 [==============================] - 32s 53us/sample - loss: 0.9488 - accuracy: 0.5976 - val_loss: 1.0054 - val_accuracy: 0.5747\n",
      "Epoch 22/40\n",
      "609675/609675 [==============================] - 39s 64us/sample - loss: 0.9464 - accuracy: 0.5985 - val_loss: 1.0079 - val_accuracy: 0.5724\n",
      "Epoch 23/40\n",
      "609675/609675 [==============================] - 34s 57us/sample - loss: 0.9440 - accuracy: 0.5995 - val_loss: 1.0020 - val_accuracy: 0.5742\n",
      "Epoch 24/40\n",
      "609675/609675 [==============================] - 33s 54us/sample - loss: 0.9416 - accuracy: 0.6002 - val_loss: 1.0117 - val_accuracy: 0.5725\n",
      "Epoch 25/40\n",
      "609675/609675 [==============================] - 36s 58us/sample - loss: 0.9393 - accuracy: 0.6009 - val_loss: 1.0049 - val_accuracy: 0.5768\n",
      "Epoch 26/40\n",
      "609675/609675 [==============================] - 35s 57us/sample - loss: 0.9371 - accuracy: 0.6019 - val_loss: 1.0174 - val_accuracy: 0.5721\n",
      "Epoch 27/40\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9346 - accuracy: 0.6035 - val_loss: 1.0130 - val_accuracy: 0.5701\n",
      "Epoch 28/40\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 0.9326 - accuracy: 0.6046 - val_loss: 1.0115 - val_accuracy: 0.5738\n",
      "Epoch 29/40\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 0.9302 - accuracy: 0.6056 - val_loss: 1.0151 - val_accuracy: 0.5708\n",
      "Epoch 30/40\n",
      "609675/609675 [==============================] - 30s 50us/sample - loss: 0.9282 - accuracy: 0.6065 - val_loss: 1.0132 - val_accuracy: 0.5752\n",
      "Epoch 31/40\n",
      "609675/609675 [==============================] - 30s 49us/sample - loss: 0.9264 - accuracy: 0.6070 - val_loss: 1.0178 - val_accuracy: 0.5733\n",
      "Epoch 32/40\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9243 - accuracy: 0.6081 - val_loss: 1.0114 - val_accuracy: 0.5738\n",
      "Epoch 33/40\n",
      "609675/609675 [==============================] - 37s 61us/sample - loss: 0.9220 - accuracy: 0.6090 - val_loss: 1.0079 - val_accuracy: 0.5736\n",
      "Epoch 34/40\n",
      "609675/609675 [==============================] - 38s 63us/sample - loss: 0.9206 - accuracy: 0.6092 - val_loss: 1.0121 - val_accuracy: 0.5757\n",
      "Epoch 35/40\n",
      "609675/609675 [==============================] - 32s 53us/sample - loss: 0.9186 - accuracy: 0.6108 - val_loss: 1.0189 - val_accuracy: 0.5699\n",
      "Epoch 36/40\n",
      "609675/609675 [==============================] - 35s 57us/sample - loss: 0.9170 - accuracy: 0.6110 - val_loss: 1.0186 - val_accuracy: 0.5759\n",
      "Epoch 37/40\n",
      "609675/609675 [==============================] - 39s 63us/sample - loss: 0.9149 - accuracy: 0.6119 - val_loss: 1.0204 - val_accuracy: 0.5736\n",
      "Epoch 38/40\n",
      "609675/609675 [==============================] - 37s 61us/sample - loss: 0.9128 - accuracy: 0.6132 - val_loss: 1.0212 - val_accuracy: 0.5733\n",
      "Epoch 39/40\n",
      "609675/609675 [==============================] - 36s 59us/sample - loss: 0.9114 - accuracy: 0.6143 - val_loss: 1.0240 - val_accuracy: 0.5723\n",
      "Epoch 40/40\n",
      "609675/609675 [==============================] - 35s 58us/sample - loss: 0.9093 - accuracy: 0.6144 - val_loss: 1.0292 - val_accuracy: 0.5745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21884c31088>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.05\n",
    "epochs=40\n",
    "batch_size=60\n",
    "\n",
    "model2_c = create_nn(lr)\n",
    "\n",
    "model2_c.fit(x_train_norm, y_train,\n",
    "           epochs=epochs, batch_size=batch_size,\n",
    "           validation_data=(x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-david",
   "metadata": {},
   "source": [
    "It stabilizes, moving between .57 and a bit over .575 after around 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "hollywood-lawyer",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 609675 samples, validate on 152419 samples\n",
      "Epoch 1/20\n",
      "609675/609675 [==============================] - 38s 62us/sample - loss: 1.0596 - accuracy: 0.5509 - val_loss: 1.0442 - val_accuracy: 0.5533\n",
      "Epoch 2/20\n",
      "609675/609675 [==============================] - 40s 65us/sample - loss: 1.0326 - accuracy: 0.5616 - val_loss: 1.0272 - val_accuracy: 0.5626\n",
      "Epoch 3/20\n",
      "609675/609675 [==============================] - 38s 63us/sample - loss: 1.0216 - accuracy: 0.5660 - val_loss: 1.0250 - val_accuracy: 0.5627\n",
      "Epoch 4/20\n",
      "609675/609675 [==============================] - 38s 62us/sample - loss: 1.0134 - accuracy: 0.5698 - val_loss: 1.0284 - val_accuracy: 0.5610\n",
      "Epoch 5/20\n",
      "609675/609675 [==============================] - 31s 51us/sample - loss: 1.0068 - accuracy: 0.5729 - val_loss: 1.0166 - val_accuracy: 0.5672\n",
      "Epoch 6/20\n",
      "609675/609675 [==============================] - 34s 55us/sample - loss: 1.0014 - accuracy: 0.5748 - val_loss: 1.0096 - val_accuracy: 0.5710\n",
      "Epoch 7/20\n",
      "609675/609675 [==============================] - 37s 60us/sample - loss: 0.9962 - accuracy: 0.5774 - val_loss: 1.0144 - val_accuracy: 0.5676\n",
      "Epoch 8/20\n",
      "609675/609675 [==============================] - 35s 58us/sample - loss: 0.9917 - accuracy: 0.5797 - val_loss: 1.0148 - val_accuracy: 0.5703\n",
      "Epoch 9/20\n",
      "609675/609675 [==============================] - 32s 52us/sample - loss: 0.9872 - accuracy: 0.5811 - val_loss: 1.0061 - val_accuracy: 0.5725\n",
      "Epoch 10/20\n",
      "609675/609675 [==============================] - 34s 56us/sample - loss: 0.9834 - accuracy: 0.5831 - val_loss: 1.0121 - val_accuracy: 0.5689\n",
      "Epoch 11/20\n",
      "609675/609675 [==============================] - 35s 57us/sample - loss: 0.9793 - accuracy: 0.5848 - val_loss: 1.0176 - val_accuracy: 0.5669\n",
      "Epoch 12/20\n",
      "609675/609675 [==============================] - 34s 56us/sample - loss: 0.9763 - accuracy: 0.5862 - val_loss: 1.0175 - val_accuracy: 0.5676\n",
      "Epoch 13/20\n",
      "609675/609675 [==============================] - 35s 58us/sample - loss: 0.9729 - accuracy: 0.5873 - val_loss: 1.0252 - val_accuracy: 0.5626\n",
      "Epoch 14/20\n",
      "609675/609675 [==============================] - 38s 62us/sample - loss: 0.9699 - accuracy: 0.5891 - val_loss: 1.0064 - val_accuracy: 0.5740\n",
      "Epoch 15/20\n",
      "609675/609675 [==============================] - 37s 61us/sample - loss: 0.9672 - accuracy: 0.5898 - val_loss: 1.0145 - val_accuracy: 0.5709\n",
      "Epoch 16/20\n",
      "609675/609675 [==============================] - 37s 60us/sample - loss: 0.9641 - accuracy: 0.5912 - val_loss: 1.0104 - val_accuracy: 0.5741\n",
      "Epoch 17/20\n",
      "609675/609675 [==============================] - 36s 60us/sample - loss: 0.9614 - accuracy: 0.5922 - val_loss: 1.0239 - val_accuracy: 0.5693\n",
      "Epoch 18/20\n",
      "609675/609675 [==============================] - 39s 65us/sample - loss: 0.9593 - accuracy: 0.5935 - val_loss: 1.0216 - val_accuracy: 0.5637\n",
      "Epoch 19/20\n",
      "609675/609675 [==============================] - 37s 60us/sample - loss: 0.9566 - accuracy: 0.5946 - val_loss: 1.0231 - val_accuracy: 0.5649\n",
      "Epoch 20/20\n",
      "609675/609675 [==============================] - 38s 62us/sample - loss: 0.9540 - accuracy: 0.5962 - val_loss: 1.0133 - val_accuracy: 0.5733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21883d585c8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.1\n",
    "epochs=20\n",
    "batch_size=60\n",
    "\n",
    "model2_e = create_nn(lr)\n",
    "\n",
    "model2_e.fit(x_train_norm, y_train,\n",
    "           epochs=epochs, batch_size=batch_size,\n",
    "           validation_data=(x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-sussex",
   "metadata": {},
   "source": [
    "Performance decreased, so the final choice is model 2 with:\n",
    "- learning rate = 0.05\n",
    "- epochs = 15\n",
    "- batch_size=60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
